{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPpWsIRBs3mq3gTLQWVHoQN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JKitsopanos/COMPUTATIONALINTELLIGENCEPERSONAL/blob/main/COMPUTATIONAL_INTELLIGENCE_COURSEWORK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bOzFc-a6QCk",
        "outputId": "dba90ba5-9190-42da-b091-438925a6e240"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 'Tesla T4')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available(), torch.cuda.get_device_name(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\" Device set to: {device}\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klYssaeme1WG",
        "outputId": "fa97cc75-2b20-4b33-8a35-f9567a9c1137"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Device set to: cuda\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deap\n",
        "\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6lF0WkRfAgP",
        "outputId": "4bf4191a-eb45-4b52-92b6-ea22f83ad327"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deap\n",
            "  Downloading deap-1.4.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from deap) (2.0.2)\n",
            "Downloading deap-1.4.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/136.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deap\n",
            "Successfully installed deap-1.4.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JKitsopanos/COMPUTATIONALINTELLIGENCEPERSONAL.git\n",
        "import os\n",
        "os.chdir(\"COMPUTATIONALINTELLIGENCEPERSONAL\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEctlowwDiaq",
        "outputId": "7845eaf8-f812-4c13-e9f8-af05d8d15284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'COMPUTATIONALINTELLIGENCEPERSONAL'...\n",
            "remote: Enumerating objects: 106, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 106 (delta 53), reused 106 (delta 53), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (106/106), 178.82 KiB | 35.76 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision matplotlib deap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khMmjzBYDogg",
        "outputId": "9b9a2b27-d0b0-488e-d944-607b53bbdd77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: deap in /usr/local/lib/python3.12/dist-packages (1.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0ABL3dQDs5h",
        "outputId": "10aec358-7fd6-46d3-f819-a89d087d5feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 'Tesla T4')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JKitsopanos/COMPUTATIONALINTELLIGENCEPERSONAL.git\n",
        "import os\n",
        "os.chdir(\"COMPUTATIONALINTELLIGENCEPERSONAL\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kwcOns3EB0i",
        "outputId": "a0060ce5-56b9-40e6-ec70-ccc136575270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'COMPUTATIONALINTELLIGENCEPERSONAL'...\n",
            "remote: Enumerating objects: 106, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 106 (delta 53), reused 106 (delta 53), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (106/106), 178.82 KiB | 14.90 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sgd.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4jjQUBDEFuR",
        "outputId": "4877a2b4-023b-4b34-fbef-19915843e15f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "100% 170M/170M [03:32<00:00, 802kB/s] \n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Starting training\n",
            "\n",
            "Epoch [1/10], Step [200/12500], Loss: 2.2942\n",
            "Epoch [1/10], Step [400/12500], Loss: 2.2391\n",
            "Epoch [1/10], Step [600/12500], Loss: 2.1450\n",
            "Epoch [1/10], Step [800/12500], Loss: 2.0433\n",
            "Epoch [1/10], Step [1000/12500], Loss: 1.9951\n",
            "Epoch [1/10], Step [1200/12500], Loss: 1.8791\n",
            "Epoch [1/10], Step [1400/12500], Loss: 1.8617\n",
            "Epoch [1/10], Step [1600/12500], Loss: 1.8156\n",
            "Epoch [1/10], Step [1800/12500], Loss: 1.8022\n",
            "Epoch [1/10], Step [2000/12500], Loss: 1.7339\n",
            "Epoch [1/10], Step [2200/12500], Loss: 1.7040\n",
            "Epoch [1/10], Step [2400/12500], Loss: 1.6506\n",
            "Epoch [1/10], Step [2600/12500], Loss: 1.6443\n",
            "Epoch [1/10], Step [2800/12500], Loss: 1.6332\n",
            "Epoch [1/10], Step [3000/12500], Loss: 1.5973\n",
            "Epoch [1/10], Step [3200/12500], Loss: 1.5604\n",
            "Epoch [1/10], Step [3400/12500], Loss: 1.5342\n",
            "Epoch [1/10], Step [3600/12500], Loss: 1.5047\n",
            "Epoch [1/10], Step [3800/12500], Loss: 1.4624\n",
            "Epoch [1/10], Step [4000/12500], Loss: 1.5066\n",
            "Epoch [1/10], Step [4200/12500], Loss: 1.4756\n",
            "Epoch [1/10], Step [4400/12500], Loss: 1.4752\n",
            "Epoch [1/10], Step [4600/12500], Loss: 1.4282\n",
            "Epoch [1/10], Step [4800/12500], Loss: 1.4069\n",
            "Epoch [1/10], Step [5000/12500], Loss: 1.4358\n",
            "Epoch [1/10], Step [5200/12500], Loss: 1.4453\n",
            "Epoch [1/10], Step [5400/12500], Loss: 1.4322\n",
            "Epoch [1/10], Step [5600/12500], Loss: 1.3818\n",
            "Epoch [1/10], Step [5800/12500], Loss: 1.3782\n",
            "Epoch [1/10], Step [6000/12500], Loss: 1.4347\n",
            "Epoch [1/10], Step [6200/12500], Loss: 1.3219\n",
            "Epoch [1/10], Step [6400/12500], Loss: 1.3405\n",
            "Epoch [1/10], Step [6600/12500], Loss: 1.3236\n",
            "Epoch [1/10], Step [6800/12500], Loss: 1.3752\n",
            "Epoch [1/10], Step [7000/12500], Loss: 1.3201\n",
            "Epoch [1/10], Step [7200/12500], Loss: 1.3140\n",
            "Epoch [1/10], Step [7400/12500], Loss: 1.2694\n",
            "Epoch [1/10], Step [7600/12500], Loss: 1.3223\n",
            "Epoch [1/10], Step [7800/12500], Loss: 1.2778\n",
            "Epoch [1/10], Step [8000/12500], Loss: 1.2468\n",
            "Epoch [1/10], Step [8200/12500], Loss: 1.3517\n",
            "Epoch [1/10], Step [8400/12500], Loss: 1.2172\n",
            "Epoch [1/10], Step [8600/12500], Loss: 1.2155\n",
            "Epoch [1/10], Step [8800/12500], Loss: 1.1716\n",
            "Epoch [1/10], Step [9000/12500], Loss: 1.2969\n",
            "Epoch [1/10], Step [9200/12500], Loss: 1.1757\n",
            "Epoch [1/10], Step [9400/12500], Loss: 1.2645\n",
            "Epoch [1/10], Step [9600/12500], Loss: 1.1495\n",
            "Epoch [1/10], Step [9800/12500], Loss: 1.1565\n",
            "Epoch [1/10], Step [10000/12500], Loss: 1.1190\n",
            "Epoch [1/10], Step [10200/12500], Loss: 1.2050\n",
            "Epoch [1/10], Step [10400/12500], Loss: 1.1830\n",
            "Epoch [1/10], Step [10600/12500], Loss: 1.1713\n",
            "Epoch [1/10], Step [10800/12500], Loss: 1.1618\n",
            "Epoch [1/10], Step [11000/12500], Loss: 1.1544\n",
            "Epoch [1/10], Step [11200/12500], Loss: 1.1416\n",
            "Epoch [1/10], Step [11400/12500], Loss: 1.0935\n",
            "Epoch [1/10], Step [11600/12500], Loss: 1.0813\n",
            "Epoch [1/10], Step [11800/12500], Loss: 1.1263\n",
            "Epoch [1/10], Step [12000/12500], Loss: 1.1216\n",
            "Epoch [1/10], Step [12200/12500], Loss: 1.1385\n",
            "Epoch [1/10], Step [12400/12500], Loss: 1.0905\n",
            "Epoch [2/10], Step [200/12500], Loss: 1.0163\n",
            "Epoch [2/10], Step [400/12500], Loss: 1.0246\n",
            "Epoch [2/10], Step [600/12500], Loss: 1.0575\n",
            "Epoch [2/10], Step [800/12500], Loss: 1.0991\n",
            "Epoch [2/10], Step [1000/12500], Loss: 0.9650\n",
            "Epoch [2/10], Step [1200/12500], Loss: 1.0479\n",
            "Epoch [2/10], Step [1400/12500], Loss: 1.0818\n",
            "Epoch [2/10], Step [1600/12500], Loss: 0.9279\n",
            "Epoch [2/10], Step [1800/12500], Loss: 1.0156\n",
            "Epoch [2/10], Step [2000/12500], Loss: 0.9723\n",
            "Epoch [2/10], Step [2200/12500], Loss: 1.0192\n",
            "Epoch [2/10], Step [2400/12500], Loss: 1.0551\n",
            "Epoch [2/10], Step [2600/12500], Loss: 0.9657\n",
            "Epoch [2/10], Step [2800/12500], Loss: 0.9435\n",
            "Epoch [2/10], Step [3000/12500], Loss: 0.9746\n",
            "Epoch [2/10], Step [3200/12500], Loss: 0.9900\n",
            "Epoch [2/10], Step [3400/12500], Loss: 0.9851\n",
            "Epoch [2/10], Step [3600/12500], Loss: 0.9369\n",
            "Epoch [2/10], Step [3800/12500], Loss: 1.0352\n",
            "Epoch [2/10], Step [4000/12500], Loss: 0.9547\n",
            "Epoch [2/10], Step [4200/12500], Loss: 0.9800\n",
            "Epoch [2/10], Step [4400/12500], Loss: 1.0265\n",
            "Epoch [2/10], Step [4600/12500], Loss: 0.9370\n",
            "Epoch [2/10], Step [4800/12500], Loss: 1.0204\n",
            "Epoch [2/10], Step [5000/12500], Loss: 0.9100\n",
            "Epoch [2/10], Step [5200/12500], Loss: 0.9513\n",
            "Epoch [2/10], Step [5400/12500], Loss: 0.8838\n",
            "Epoch [2/10], Step [5600/12500], Loss: 0.9296\n",
            "Epoch [2/10], Step [5800/12500], Loss: 0.9375\n",
            "Epoch [2/10], Step [6000/12500], Loss: 0.9288\n",
            "Epoch [2/10], Step [6200/12500], Loss: 0.9209\n",
            "Epoch [2/10], Step [6400/12500], Loss: 0.8887\n",
            "Epoch [2/10], Step [6600/12500], Loss: 0.9810\n",
            "Epoch [2/10], Step [6800/12500], Loss: 0.9660\n",
            "Epoch [2/10], Step [7000/12500], Loss: 0.9109\n",
            "Epoch [2/10], Step [7200/12500], Loss: 0.9221\n",
            "Epoch [2/10], Step [7400/12500], Loss: 0.9024\n",
            "Epoch [2/10], Step [7600/12500], Loss: 0.8761\n",
            "Epoch [2/10], Step [7800/12500], Loss: 0.9216\n",
            "Epoch [2/10], Step [8000/12500], Loss: 0.8598\n",
            "Epoch [2/10], Step [8200/12500], Loss: 0.9300\n",
            "Epoch [2/10], Step [8400/12500], Loss: 0.9435\n",
            "Epoch [2/10], Step [8600/12500], Loss: 0.9230\n",
            "Epoch [2/10], Step [8800/12500], Loss: 0.8490\n",
            "Epoch [2/10], Step [9000/12500], Loss: 0.8819\n",
            "Epoch [2/10], Step [9200/12500], Loss: 0.8634\n",
            "Epoch [2/10], Step [9400/12500], Loss: 1.0074\n",
            "Epoch [2/10], Step [9600/12500], Loss: 0.8508\n",
            "Epoch [2/10], Step [9800/12500], Loss: 0.8645\n",
            "Epoch [2/10], Step [10000/12500], Loss: 0.8535\n",
            "Epoch [2/10], Step [10200/12500], Loss: 0.9137\n",
            "Epoch [2/10], Step [10400/12500], Loss: 0.8639\n",
            "Epoch [2/10], Step [10600/12500], Loss: 0.9180\n",
            "Epoch [2/10], Step [10800/12500], Loss: 0.9448\n",
            "Epoch [2/10], Step [11000/12500], Loss: 0.9174\n",
            "Epoch [2/10], Step [11200/12500], Loss: 0.8697\n",
            "Epoch [2/10], Step [11400/12500], Loss: 0.8348\n",
            "Epoch [2/10], Step [11600/12500], Loss: 0.8841\n",
            "Epoch [2/10], Step [11800/12500], Loss: 0.9559\n",
            "Epoch [2/10], Step [12000/12500], Loss: 0.8546\n",
            "Epoch [2/10], Step [12200/12500], Loss: 0.9261\n",
            "Epoch [2/10], Step [12400/12500], Loss: 0.8478\n",
            "Epoch [3/10], Step [200/12500], Loss: 0.7582\n",
            "Epoch [3/10], Step [400/12500], Loss: 0.7486\n",
            "Epoch [3/10], Step [600/12500], Loss: 0.6742\n",
            "Epoch [3/10], Step [800/12500], Loss: 0.7699\n",
            "Epoch [3/10], Step [1000/12500], Loss: 0.6810\n",
            "Epoch [3/10], Step [1200/12500], Loss: 0.6625\n",
            "Epoch [3/10], Step [1400/12500], Loss: 0.7455\n",
            "Epoch [3/10], Step [1600/12500], Loss: 0.7155\n",
            "Epoch [3/10], Step [1800/12500], Loss: 0.7280\n",
            "Epoch [3/10], Step [2000/12500], Loss: 0.7055\n",
            "Epoch [3/10], Step [2200/12500], Loss: 0.7447\n",
            "Epoch [3/10], Step [2400/12500], Loss: 0.7255\n",
            "Epoch [3/10], Step [2600/12500], Loss: 0.8105\n",
            "Epoch [3/10], Step [2800/12500], Loss: 0.7227\n",
            "Epoch [3/10], Step [3000/12500], Loss: 0.7343\n",
            "Epoch [3/10], Step [3200/12500], Loss: 0.7025\n",
            "Epoch [3/10], Step [3400/12500], Loss: 0.7692\n",
            "Epoch [3/10], Step [3600/12500], Loss: 0.6822\n",
            "Epoch [3/10], Step [3800/12500], Loss: 0.7445\n",
            "Epoch [3/10], Step [4000/12500], Loss: 0.7426\n",
            "Epoch [3/10], Step [4200/12500], Loss: 0.7540\n",
            "Epoch [3/10], Step [4400/12500], Loss: 0.7352\n",
            "Epoch [3/10], Step [4600/12500], Loss: 0.7018\n",
            "Epoch [3/10], Step [4800/12500], Loss: 0.7164\n",
            "Epoch [3/10], Step [5000/12500], Loss: 0.7577\n",
            "Epoch [3/10], Step [5200/12500], Loss: 0.7702\n",
            "Epoch [3/10], Step [5400/12500], Loss: 0.6872\n",
            "Epoch [3/10], Step [5600/12500], Loss: 0.6867\n",
            "Epoch [3/10], Step [5800/12500], Loss: 0.7424\n",
            "Epoch [3/10], Step [6000/12500], Loss: 0.7123\n",
            "Epoch [3/10], Step [6200/12500], Loss: 0.6979\n",
            "Epoch [3/10], Step [6400/12500], Loss: 0.7754\n",
            "Epoch [3/10], Step [6600/12500], Loss: 0.7317\n",
            "Epoch [3/10], Step [6800/12500], Loss: 0.6889\n",
            "Epoch [3/10], Step [7000/12500], Loss: 0.6844\n",
            "Epoch [3/10], Step [7200/12500], Loss: 0.7275\n",
            "Epoch [3/10], Step [7400/12500], Loss: 0.7390\n",
            "Epoch [3/10], Step [7600/12500], Loss: 0.7617\n",
            "Epoch [3/10], Step [7800/12500], Loss: 0.7085\n",
            "Epoch [3/10], Step [8000/12500], Loss: 0.7500\n",
            "Epoch [3/10], Step [8200/12500], Loss: 0.6758\n",
            "Epoch [3/10], Step [8400/12500], Loss: 0.7148\n",
            "Epoch [3/10], Step [8600/12500], Loss: 0.6864\n",
            "Epoch [3/10], Step [8800/12500], Loss: 0.7001\n",
            "Epoch [3/10], Step [9000/12500], Loss: 0.7248\n",
            "Epoch [3/10], Step [9200/12500], Loss: 0.6816\n",
            "Epoch [3/10], Step [9400/12500], Loss: 0.6575\n",
            "Epoch [3/10], Step [9600/12500], Loss: 0.7134\n",
            "Epoch [3/10], Step [9800/12500], Loss: 0.7476\n",
            "Epoch [3/10], Step [10000/12500], Loss: 0.6626\n",
            "Epoch [3/10], Step [10200/12500], Loss: 0.7218\n",
            "Epoch [3/10], Step [10400/12500], Loss: 0.7421\n",
            "Epoch [3/10], Step [10600/12500], Loss: 0.6662\n",
            "Epoch [3/10], Step [10800/12500], Loss: 0.6131\n",
            "Epoch [3/10], Step [11000/12500], Loss: 0.7122\n",
            "Epoch [3/10], Step [11200/12500], Loss: 0.6888\n",
            "Epoch [3/10], Step [11400/12500], Loss: 0.6963\n",
            "Epoch [3/10], Step [11600/12500], Loss: 0.6787\n",
            "Epoch [3/10], Step [11800/12500], Loss: 0.7268\n",
            "Epoch [3/10], Step [12000/12500], Loss: 0.6762\n",
            "Epoch [3/10], Step [12200/12500], Loss: 0.6522\n",
            "Epoch [3/10], Step [12400/12500], Loss: 0.7049\n",
            "Epoch [4/10], Step [200/12500], Loss: 0.4818\n",
            "Epoch [4/10], Step [400/12500], Loss: 0.5206\n",
            "Epoch [4/10], Step [600/12500], Loss: 0.4712\n",
            "Epoch [4/10], Step [800/12500], Loss: 0.4592\n",
            "Epoch [4/10], Step [1000/12500], Loss: 0.4810\n",
            "Epoch [4/10], Step [1200/12500], Loss: 0.5295\n",
            "Epoch [4/10], Step [1400/12500], Loss: 0.5099\n",
            "Epoch [4/10], Step [1600/12500], Loss: 0.5237\n",
            "Epoch [4/10], Step [1800/12500], Loss: 0.5330\n",
            "Epoch [4/10], Step [2000/12500], Loss: 0.5278\n",
            "Epoch [4/10], Step [2200/12500], Loss: 0.5370\n",
            "Epoch [4/10], Step [2400/12500], Loss: 0.4988\n",
            "Epoch [4/10], Step [2600/12500], Loss: 0.5452\n",
            "Epoch [4/10], Step [2800/12500], Loss: 0.5175\n",
            "Epoch [4/10], Step [3000/12500], Loss: 0.4787\n",
            "Epoch [4/10], Step [3200/12500], Loss: 0.4441\n",
            "Epoch [4/10], Step [3400/12500], Loss: 0.5191\n",
            "Epoch [4/10], Step [3600/12500], Loss: 0.4552\n",
            "Epoch [4/10], Step [3800/12500], Loss: 0.5175\n",
            "Epoch [4/10], Step [4000/12500], Loss: 0.5046\n",
            "Epoch [4/10], Step [4200/12500], Loss: 0.4858\n",
            "Epoch [4/10], Step [4400/12500], Loss: 0.5750\n",
            "Epoch [4/10], Step [4600/12500], Loss: 0.4932\n",
            "Epoch [4/10], Step [4800/12500], Loss: 0.5268\n",
            "Epoch [4/10], Step [5000/12500], Loss: 0.5066\n",
            "Epoch [4/10], Step [5200/12500], Loss: 0.4328\n",
            "Epoch [4/10], Step [5400/12500], Loss: 0.5160\n",
            "Epoch [4/10], Step [5600/12500], Loss: 0.4847\n",
            "Epoch [4/10], Step [5800/12500], Loss: 0.5151\n",
            "Epoch [4/10], Step [6000/12500], Loss: 0.5834\n",
            "Epoch [4/10], Step [6200/12500], Loss: 0.5390\n",
            "Epoch [4/10], Step [6400/12500], Loss: 0.5293\n",
            "Epoch [4/10], Step [6600/12500], Loss: 0.5624\n",
            "Epoch [4/10], Step [6800/12500], Loss: 0.4791\n",
            "Epoch [4/10], Step [7000/12500], Loss: 0.5333\n",
            "Epoch [4/10], Step [7200/12500], Loss: 0.4632\n",
            "Epoch [4/10], Step [7400/12500], Loss: 0.5185\n",
            "Epoch [4/10], Step [7600/12500], Loss: 0.5307\n",
            "Epoch [4/10], Step [7800/12500], Loss: 0.4612\n",
            "Epoch [4/10], Step [8000/12500], Loss: 0.5192\n",
            "Epoch [4/10], Step [8200/12500], Loss: 0.4983\n",
            "Epoch [4/10], Step [8400/12500], Loss: 0.5155\n",
            "Epoch [4/10], Step [8600/12500], Loss: 0.5583\n",
            "Epoch [4/10], Step [8800/12500], Loss: 0.4835\n",
            "Epoch [4/10], Step [9000/12500], Loss: 0.5959\n",
            "Epoch [4/10], Step [9200/12500], Loss: 0.5152\n",
            "Epoch [4/10], Step [9400/12500], Loss: 0.5325\n",
            "Epoch [4/10], Step [9600/12500], Loss: 0.5870\n",
            "Epoch [4/10], Step [9800/12500], Loss: 0.5882\n",
            "Epoch [4/10], Step [10000/12500], Loss: 0.6155\n",
            "Epoch [4/10], Step [10200/12500], Loss: 0.4936\n",
            "Epoch [4/10], Step [10400/12500], Loss: 0.5393\n",
            "Epoch [4/10], Step [10600/12500], Loss: 0.4573\n",
            "Epoch [4/10], Step [10800/12500], Loss: 0.5551\n",
            "Epoch [4/10], Step [11000/12500], Loss: 0.5682\n",
            "Epoch [4/10], Step [11200/12500], Loss: 0.5473\n",
            "Epoch [4/10], Step [11400/12500], Loss: 0.5485\n",
            "Epoch [4/10], Step [11600/12500], Loss: 0.5440\n",
            "Epoch [4/10], Step [11800/12500], Loss: 0.5586\n",
            "Epoch [4/10], Step [12000/12500], Loss: 0.5287\n",
            "Epoch [4/10], Step [12200/12500], Loss: 0.5417\n",
            "Epoch [4/10], Step [12400/12500], Loss: 0.5476\n",
            "Epoch [5/10], Step [200/12500], Loss: 0.3028\n",
            "Epoch [5/10], Step [400/12500], Loss: 0.3122\n",
            "Epoch [5/10], Step [600/12500], Loss: 0.2747\n",
            "Epoch [5/10], Step [800/12500], Loss: 0.2788\n",
            "Epoch [5/10], Step [1000/12500], Loss: 0.2807\n",
            "Epoch [5/10], Step [1200/12500], Loss: 0.2833\n",
            "Epoch [5/10], Step [1400/12500], Loss: 0.2970\n",
            "Epoch [5/10], Step [1600/12500], Loss: 0.2761\n",
            "Epoch [5/10], Step [1800/12500], Loss: 0.2806\n",
            "Epoch [5/10], Step [2000/12500], Loss: 0.2614\n",
            "Epoch [5/10], Step [2200/12500], Loss: 0.2854\n",
            "Epoch [5/10], Step [2400/12500], Loss: 0.2801\n",
            "Epoch [5/10], Step [2600/12500], Loss: 0.2834\n",
            "Epoch [5/10], Step [2800/12500], Loss: 0.3129\n",
            "Epoch [5/10], Step [3000/12500], Loss: 0.3039\n",
            "Epoch [5/10], Step [3200/12500], Loss: 0.3207\n",
            "Epoch [5/10], Step [3400/12500], Loss: 0.3182\n",
            "Epoch [5/10], Step [3600/12500], Loss: 0.3519\n",
            "Epoch [5/10], Step [3800/12500], Loss: 0.2845\n",
            "Epoch [5/10], Step [4000/12500], Loss: 0.3022\n",
            "Epoch [5/10], Step [4200/12500], Loss: 0.3442\n",
            "Epoch [5/10], Step [4400/12500], Loss: 0.3535\n",
            "Epoch [5/10], Step [4600/12500], Loss: 0.3175\n",
            "Epoch [5/10], Step [4800/12500], Loss: 0.3554\n",
            "Epoch [5/10], Step [5000/12500], Loss: 0.3614\n",
            "Epoch [5/10], Step [5200/12500], Loss: 0.2990\n",
            "Epoch [5/10], Step [5400/12500], Loss: 0.3624\n",
            "Epoch [5/10], Step [5600/12500], Loss: 0.3108\n",
            "Epoch [5/10], Step [5800/12500], Loss: 0.3163\n",
            "Epoch [5/10], Step [6000/12500], Loss: 0.3413\n",
            "Epoch [5/10], Step [6200/12500], Loss: 0.2975\n",
            "Epoch [5/10], Step [6400/12500], Loss: 0.3346\n",
            "Epoch [5/10], Step [6600/12500], Loss: 0.3724\n",
            "Epoch [5/10], Step [6800/12500], Loss: 0.3578\n",
            "Epoch [5/10], Step [7000/12500], Loss: 0.3278\n",
            "Epoch [5/10], Step [7200/12500], Loss: 0.3217\n",
            "Epoch [5/10], Step [7400/12500], Loss: 0.3591\n",
            "Epoch [5/10], Step [7600/12500], Loss: 0.3981\n",
            "Epoch [5/10], Step [7800/12500], Loss: 0.3558\n",
            "Epoch [5/10], Step [8000/12500], Loss: 0.3910\n",
            "Epoch [5/10], Step [8200/12500], Loss: 0.3687\n",
            "Epoch [5/10], Step [8400/12500], Loss: 0.3068\n",
            "Epoch [5/10], Step [8600/12500], Loss: 0.3298\n",
            "Epoch [5/10], Step [8800/12500], Loss: 0.2897\n",
            "Epoch [5/10], Step [9000/12500], Loss: 0.4240\n",
            "Epoch [5/10], Step [9200/12500], Loss: 0.3809\n",
            "Epoch [5/10], Step [9400/12500], Loss: 0.3757\n",
            "Epoch [5/10], Step [9600/12500], Loss: 0.3317\n",
            "Epoch [5/10], Step [9800/12500], Loss: 0.3063\n",
            "Epoch [5/10], Step [10000/12500], Loss: 0.3862\n",
            "Epoch [5/10], Step [10200/12500], Loss: 0.3503\n",
            "Epoch [5/10], Step [10400/12500], Loss: 0.3973\n",
            "Epoch [5/10], Step [10600/12500], Loss: 0.4326\n",
            "Epoch [5/10], Step [10800/12500], Loss: 0.3683\n",
            "Epoch [5/10], Step [11000/12500], Loss: 0.3664\n",
            "Epoch [5/10], Step [11200/12500], Loss: 0.3456\n",
            "Epoch [5/10], Step [11400/12500], Loss: 0.3476\n",
            "Epoch [5/10], Step [11600/12500], Loss: 0.3825\n",
            "Epoch [5/10], Step [11800/12500], Loss: 0.3825\n",
            "Epoch [5/10], Step [12000/12500], Loss: 0.3893\n",
            "Epoch [5/10], Step [12200/12500], Loss: 0.3276\n",
            "Epoch [5/10], Step [12400/12500], Loss: 0.3280\n",
            "Epoch [6/10], Step [200/12500], Loss: 0.1662\n",
            "Epoch [6/10], Step [400/12500], Loss: 0.1625\n",
            "Epoch [6/10], Step [600/12500], Loss: 0.1336\n",
            "Epoch [6/10], Step [800/12500], Loss: 0.1555\n",
            "Epoch [6/10], Step [1000/12500], Loss: 0.1323\n",
            "Epoch [6/10], Step [1200/12500], Loss: 0.1666\n",
            "Epoch [6/10], Step [1400/12500], Loss: 0.1252\n",
            "Epoch [6/10], Step [1600/12500], Loss: 0.1293\n",
            "Epoch [6/10], Step [1800/12500], Loss: 0.1494\n",
            "Epoch [6/10], Step [2000/12500], Loss: 0.1525\n",
            "Epoch [6/10], Step [2200/12500], Loss: 0.1892\n",
            "Epoch [6/10], Step [2400/12500], Loss: 0.1813\n",
            "Epoch [6/10], Step [2600/12500], Loss: 0.1906\n",
            "Epoch [6/10], Step [2800/12500], Loss: 0.1999\n",
            "Epoch [6/10], Step [3000/12500], Loss: 0.1723\n",
            "Epoch [6/10], Step [3200/12500], Loss: 0.1881\n",
            "Epoch [6/10], Step [3400/12500], Loss: 0.2023\n",
            "Epoch [6/10], Step [3600/12500], Loss: 0.1806\n",
            "Epoch [6/10], Step [3800/12500], Loss: 0.1656\n",
            "Epoch [6/10], Step [4000/12500], Loss: 0.1649\n",
            "Epoch [6/10], Step [4200/12500], Loss: 0.1684\n",
            "Epoch [6/10], Step [4400/12500], Loss: 0.1801\n",
            "Epoch [6/10], Step [4600/12500], Loss: 0.1590\n",
            "Epoch [6/10], Step [4800/12500], Loss: 0.1418\n",
            "Epoch [6/10], Step [5000/12500], Loss: 0.1946\n",
            "Epoch [6/10], Step [5200/12500], Loss: 0.2056\n",
            "Epoch [6/10], Step [5400/12500], Loss: 0.1976\n",
            "Epoch [6/10], Step [5600/12500], Loss: 0.1602\n",
            "Epoch [6/10], Step [5800/12500], Loss: 0.1907\n",
            "Epoch [6/10], Step [6000/12500], Loss: 0.1704\n",
            "Epoch [6/10], Step [6200/12500], Loss: 0.2024\n",
            "Epoch [6/10], Step [6400/12500], Loss: 0.2253\n",
            "Epoch [6/10], Step [6600/12500], Loss: 0.1880\n",
            "Epoch [6/10], Step [6800/12500], Loss: 0.2283\n",
            "Epoch [6/10], Step [7000/12500], Loss: 0.1711\n",
            "Epoch [6/10], Step [7200/12500], Loss: 0.2085\n",
            "Epoch [6/10], Step [7400/12500], Loss: 0.2275\n",
            "Epoch [6/10], Step [7600/12500], Loss: 0.2090\n",
            "Epoch [6/10], Step [7800/12500], Loss: 0.2001\n",
            "Epoch [6/10], Step [8000/12500], Loss: 0.2166\n",
            "Epoch [6/10], Step [8200/12500], Loss: 0.1921\n",
            "Epoch [6/10], Step [8400/12500], Loss: 0.1707\n",
            "Epoch [6/10], Step [8600/12500], Loss: 0.2161\n",
            "Epoch [6/10], Step [8800/12500], Loss: 0.2324\n",
            "Epoch [6/10], Step [9000/12500], Loss: 0.2640\n",
            "Epoch [6/10], Step [9200/12500], Loss: 0.2068\n",
            "Epoch [6/10], Step [9400/12500], Loss: 0.2007\n",
            "Epoch [6/10], Step [9600/12500], Loss: 0.2448\n",
            "Epoch [6/10], Step [9800/12500], Loss: 0.2224\n",
            "Epoch [6/10], Step [10000/12500], Loss: 0.2154\n",
            "Epoch [6/10], Step [10200/12500], Loss: 0.2093\n",
            "Epoch [6/10], Step [10400/12500], Loss: 0.2644\n",
            "Epoch [6/10], Step [10600/12500], Loss: 0.2278\n",
            "Epoch [6/10], Step [10800/12500], Loss: 0.2328\n",
            "Epoch [6/10], Step [11000/12500], Loss: 0.2136\n",
            "Epoch [6/10], Step [11200/12500], Loss: 0.2638\n",
            "Epoch [6/10], Step [11400/12500], Loss: 0.2283\n",
            "Epoch [6/10], Step [11600/12500], Loss: 0.2261\n",
            "Epoch [6/10], Step [11800/12500], Loss: 0.2352\n",
            "Epoch [6/10], Step [12000/12500], Loss: 0.2197\n",
            "Epoch [6/10], Step [12200/12500], Loss: 0.2969\n",
            "Epoch [6/10], Step [12400/12500], Loss: 0.2522\n",
            "Epoch [7/10], Step [200/12500], Loss: 0.0910\n",
            "Epoch [7/10], Step [400/12500], Loss: 0.0841\n",
            "Epoch [7/10], Step [600/12500], Loss: 0.0764\n",
            "Epoch [7/10], Step [800/12500], Loss: 0.0861\n",
            "Epoch [7/10], Step [1000/12500], Loss: 0.0999\n",
            "Epoch [7/10], Step [1200/12500], Loss: 0.0802\n",
            "Epoch [7/10], Step [1400/12500], Loss: 0.0752\n",
            "Epoch [7/10], Step [1600/12500], Loss: 0.0912\n",
            "Epoch [7/10], Step [1800/12500], Loss: 0.0751\n",
            "Epoch [7/10], Step [2000/12500], Loss: 0.0870\n",
            "Epoch [7/10], Step [2200/12500], Loss: 0.1383\n",
            "Epoch [7/10], Step [2400/12500], Loss: 0.0978\n",
            "Epoch [7/10], Step [2600/12500], Loss: 0.1234\n",
            "Epoch [7/10], Step [2800/12500], Loss: 0.1044\n",
            "Epoch [7/10], Step [3000/12500], Loss: 0.1068\n",
            "Epoch [7/10], Step [3200/12500], Loss: 0.0910\n",
            "Epoch [7/10], Step [3400/12500], Loss: 0.1044\n",
            "Epoch [7/10], Step [3600/12500], Loss: 0.0982\n",
            "Epoch [7/10], Step [3800/12500], Loss: 0.0892\n",
            "Epoch [7/10], Step [4000/12500], Loss: 0.1415\n",
            "Epoch [7/10], Step [4200/12500], Loss: 0.1287\n",
            "Epoch [7/10], Step [4400/12500], Loss: 0.1135\n",
            "Epoch [7/10], Step [4600/12500], Loss: 0.1337\n",
            "Epoch [7/10], Step [4800/12500], Loss: 0.1199\n",
            "Epoch [7/10], Step [5000/12500], Loss: 0.1142\n",
            "Epoch [7/10], Step [5200/12500], Loss: 0.1010\n",
            "Epoch [7/10], Step [5400/12500], Loss: 0.1278\n",
            "Epoch [7/10], Step [5600/12500], Loss: 0.1085\n",
            "Epoch [7/10], Step [5800/12500], Loss: 0.1008\n",
            "Epoch [7/10], Step [6000/12500], Loss: 0.1164\n",
            "Epoch [7/10], Step [6200/12500], Loss: 0.1628\n",
            "Epoch [7/10], Step [6400/12500], Loss: 0.1126\n",
            "Epoch [7/10], Step [6600/12500], Loss: 0.1172\n",
            "Epoch [7/10], Step [6800/12500], Loss: 0.0984\n",
            "Epoch [7/10], Step [7000/12500], Loss: 0.1229\n",
            "Epoch [7/10], Step [7200/12500], Loss: 0.1241\n",
            "Epoch [7/10], Step [7400/12500], Loss: 0.1417\n",
            "Epoch [7/10], Step [7600/12500], Loss: 0.1204\n",
            "Epoch [7/10], Step [7800/12500], Loss: 0.1408\n",
            "Epoch [7/10], Step [8000/12500], Loss: 0.1692\n",
            "Epoch [7/10], Step [8200/12500], Loss: 0.1752\n",
            "Epoch [7/10], Step [8400/12500], Loss: 0.0990\n",
            "Epoch [7/10], Step [8600/12500], Loss: 0.1456\n",
            "Epoch [7/10], Step [8800/12500], Loss: 0.1372\n",
            "Epoch [7/10], Step [9000/12500], Loss: 0.1486\n",
            "Epoch [7/10], Step [9200/12500], Loss: 0.1202\n",
            "Epoch [7/10], Step [9400/12500], Loss: 0.1699\n",
            "Epoch [7/10], Step [9600/12500], Loss: 0.1546\n",
            "Epoch [7/10], Step [9800/12500], Loss: 0.1803\n",
            "Epoch [7/10], Step [10000/12500], Loss: 0.1088\n",
            "Epoch [7/10], Step [10200/12500], Loss: 0.1456\n",
            "Epoch [7/10], Step [10400/12500], Loss: 0.1236\n",
            "Epoch [7/10], Step [10600/12500], Loss: 0.1698\n",
            "Epoch [7/10], Step [10800/12500], Loss: 0.1431\n",
            "Epoch [7/10], Step [11000/12500], Loss: 0.1325\n",
            "Epoch [7/10], Step [11200/12500], Loss: 0.1271\n",
            "Epoch [7/10], Step [11400/12500], Loss: 0.1421\n",
            "Epoch [7/10], Step [11600/12500], Loss: 0.1838\n",
            "Epoch [7/10], Step [11800/12500], Loss: 0.1608\n",
            "Epoch [7/10], Step [12000/12500], Loss: 0.1985\n",
            "Epoch [7/10], Step [12200/12500], Loss: 0.1939\n",
            "Epoch [7/10], Step [12400/12500], Loss: 0.2003\n",
            "Epoch [8/10], Step [200/12500], Loss: 0.0657\n",
            "Epoch [8/10], Step [400/12500], Loss: 0.0752\n",
            "Epoch [8/10], Step [600/12500], Loss: 0.0983\n",
            "Epoch [8/10], Step [800/12500], Loss: 0.0557\n",
            "Epoch [8/10], Step [1000/12500], Loss: 0.0770\n",
            "Epoch [8/10], Step [1200/12500], Loss: 0.0712\n",
            "Epoch [8/10], Step [1400/12500], Loss: 0.0822\n",
            "Epoch [8/10], Step [1600/12500], Loss: 0.0721\n",
            "Epoch [8/10], Step [1800/12500], Loss: 0.0612\n",
            "Epoch [8/10], Step [2000/12500], Loss: 0.0461\n",
            "Epoch [8/10], Step [2200/12500], Loss: 0.0747\n",
            "Epoch [8/10], Step [2400/12500], Loss: 0.0637\n",
            "Epoch [8/10], Step [2600/12500], Loss: 0.0789\n",
            "Epoch [8/10], Step [2800/12500], Loss: 0.0741\n",
            "Epoch [8/10], Step [3000/12500], Loss: 0.0486\n",
            "Epoch [8/10], Step [3200/12500], Loss: 0.0764\n",
            "Epoch [8/10], Step [3400/12500], Loss: 0.0460\n",
            "Epoch [8/10], Step [3600/12500], Loss: 0.0482\n",
            "Epoch [8/10], Step [3800/12500], Loss: 0.0775\n",
            "Epoch [8/10], Step [4000/12500], Loss: 0.1110\n",
            "Epoch [8/10], Step [4200/12500], Loss: 0.0856\n",
            "Epoch [8/10], Step [4400/12500], Loss: 0.0678\n",
            "Epoch [8/10], Step [4600/12500], Loss: 0.0493\n",
            "Epoch [8/10], Step [4800/12500], Loss: 0.0836\n",
            "Epoch [8/10], Step [5000/12500], Loss: 0.0906\n",
            "Epoch [8/10], Step [5200/12500], Loss: 0.1105\n",
            "Epoch [8/10], Step [5400/12500], Loss: 0.1172\n",
            "Epoch [8/10], Step [5600/12500], Loss: 0.0732\n",
            "Epoch [8/10], Step [5800/12500], Loss: 0.0812\n",
            "Epoch [8/10], Step [6000/12500], Loss: 0.0703\n",
            "Epoch [8/10], Step [6200/12500], Loss: 0.0525\n",
            "Epoch [8/10], Step [6400/12500], Loss: 0.0840\n",
            "Epoch [8/10], Step [6600/12500], Loss: 0.0876\n",
            "Epoch [8/10], Step [6800/12500], Loss: 0.0714\n",
            "Epoch [8/10], Step [7000/12500], Loss: 0.1270\n",
            "Epoch [8/10], Step [7200/12500], Loss: 0.1272\n",
            "Epoch [8/10], Step [7400/12500], Loss: 0.1069\n",
            "Epoch [8/10], Step [7600/12500], Loss: 0.1220\n",
            "Epoch [8/10], Step [7800/12500], Loss: 0.0941\n",
            "Epoch [8/10], Step [8000/12500], Loss: 0.0921\n",
            "Epoch [8/10], Step [8200/12500], Loss: 0.0846\n",
            "Epoch [8/10], Step [8400/12500], Loss: 0.1001\n",
            "Epoch [8/10], Step [8600/12500], Loss: 0.0755\n",
            "Epoch [8/10], Step [8800/12500], Loss: 0.1153\n",
            "Epoch [8/10], Step [9000/12500], Loss: 0.1147\n",
            "Epoch [8/10], Step [9200/12500], Loss: 0.0924\n",
            "Epoch [8/10], Step [9400/12500], Loss: 0.1082\n",
            "Epoch [8/10], Step [9600/12500], Loss: 0.0764\n",
            "Epoch [8/10], Step [9800/12500], Loss: 0.1006\n",
            "Epoch [8/10], Step [10000/12500], Loss: 0.1105\n",
            "Epoch [8/10], Step [10200/12500], Loss: 0.0821\n",
            "Epoch [8/10], Step [10400/12500], Loss: 0.1211\n",
            "Epoch [8/10], Step [10600/12500], Loss: 0.1217\n",
            "Epoch [8/10], Step [10800/12500], Loss: 0.1037\n",
            "Epoch [8/10], Step [11000/12500], Loss: 0.1108\n",
            "Epoch [8/10], Step [11200/12500], Loss: 0.0846\n",
            "Epoch [8/10], Step [11400/12500], Loss: 0.1040\n",
            "Epoch [8/10], Step [11600/12500], Loss: 0.0681\n",
            "Epoch [8/10], Step [11800/12500], Loss: 0.1272\n",
            "Epoch [8/10], Step [12000/12500], Loss: 0.0991\n",
            "Epoch [8/10], Step [12200/12500], Loss: 0.1661\n",
            "Epoch [8/10], Step [12400/12500], Loss: 0.1037\n",
            "Epoch [9/10], Step [200/12500], Loss: 0.0537\n",
            "Epoch [9/10], Step [400/12500], Loss: 0.0602\n",
            "Epoch [9/10], Step [600/12500], Loss: 0.0814\n",
            "Epoch [9/10], Step [800/12500], Loss: 0.0561\n",
            "Epoch [9/10], Step [1000/12500], Loss: 0.0578\n",
            "Epoch [9/10], Step [1200/12500], Loss: 0.0345\n",
            "Epoch [9/10], Step [1400/12500], Loss: 0.0430\n",
            "Epoch [9/10], Step [1600/12500], Loss: 0.0766\n",
            "Epoch [9/10], Step [1800/12500], Loss: 0.0865\n",
            "Epoch [9/10], Step [2000/12500], Loss: 0.0360\n",
            "Epoch [9/10], Step [2200/12500], Loss: 0.0447\n",
            "Epoch [9/10], Step [2400/12500], Loss: 0.0578\n",
            "Epoch [9/10], Step [2600/12500], Loss: 0.0564\n",
            "Epoch [9/10], Step [2800/12500], Loss: 0.0623\n",
            "Epoch [9/10], Step [3000/12500], Loss: 0.0542\n",
            "Epoch [9/10], Step [3200/12500], Loss: 0.0730\n",
            "Epoch [9/10], Step [3400/12500], Loss: 0.0438\n",
            "Epoch [9/10], Step [3600/12500], Loss: 0.0398\n",
            "Epoch [9/10], Step [3800/12500], Loss: 0.0533\n",
            "Epoch [9/10], Step [4000/12500], Loss: 0.0252\n",
            "Epoch [9/10], Step [4200/12500], Loss: 0.0455\n",
            "Epoch [9/10], Step [4400/12500], Loss: 0.0209\n",
            "Epoch [9/10], Step [4600/12500], Loss: 0.0286\n",
            "Epoch [9/10], Step [4800/12500], Loss: 0.0624\n",
            "Epoch [9/10], Step [5000/12500], Loss: 0.0429\n",
            "Epoch [9/10], Step [5200/12500], Loss: 0.0525\n",
            "Epoch [9/10], Step [5400/12500], Loss: 0.0470\n",
            "Epoch [9/10], Step [5600/12500], Loss: 0.1041\n",
            "Epoch [9/10], Step [5800/12500], Loss: 0.0855\n",
            "Epoch [9/10], Step [6000/12500], Loss: 0.0936\n",
            "Epoch [9/10], Step [6200/12500], Loss: 0.0800\n",
            "Epoch [9/10], Step [6400/12500], Loss: 0.0543\n",
            "Epoch [9/10], Step [6600/12500], Loss: 0.0683\n",
            "Epoch [9/10], Step [6800/12500], Loss: 0.0943\n",
            "Epoch [9/10], Step [7000/12500], Loss: 0.0412\n",
            "Epoch [9/10], Step [7200/12500], Loss: 0.0444\n",
            "Epoch [9/10], Step [7400/12500], Loss: 0.0459\n",
            "Epoch [9/10], Step [7600/12500], Loss: 0.0836\n",
            "Epoch [9/10], Step [7800/12500], Loss: 0.0610\n",
            "Epoch [9/10], Step [8000/12500], Loss: 0.0612\n",
            "Epoch [9/10], Step [8200/12500], Loss: 0.0367\n",
            "Epoch [9/10], Step [8400/12500], Loss: 0.0492\n",
            "Epoch [9/10], Step [8600/12500], Loss: 0.0552\n",
            "Epoch [9/10], Step [8800/12500], Loss: 0.0777\n",
            "Epoch [9/10], Step [9000/12500], Loss: 0.0780\n",
            "Epoch [9/10], Step [9200/12500], Loss: 0.0863\n",
            "Epoch [9/10], Step [9400/12500], Loss: 0.0636\n",
            "Epoch [9/10], Step [9600/12500], Loss: 0.0585\n",
            "Epoch [9/10], Step [9800/12500], Loss: 0.1410\n",
            "Epoch [9/10], Step [10000/12500], Loss: 0.0648\n",
            "Epoch [9/10], Step [10200/12500], Loss: 0.0732\n",
            "Epoch [9/10], Step [10400/12500], Loss: 0.0693\n",
            "Epoch [9/10], Step [10600/12500], Loss: 0.0709\n",
            "Epoch [9/10], Step [10800/12500], Loss: 0.0592\n",
            "Epoch [9/10], Step [11000/12500], Loss: 0.0560\n",
            "Epoch [9/10], Step [11200/12500], Loss: 0.0389\n",
            "Epoch [9/10], Step [11400/12500], Loss: 0.0564\n",
            "Epoch [9/10], Step [11600/12500], Loss: 0.1053\n",
            "Epoch [9/10], Step [11800/12500], Loss: 0.0864\n",
            "Epoch [9/10], Step [12000/12500], Loss: 0.0899\n",
            "Epoch [9/10], Step [12200/12500], Loss: 0.0959\n",
            "Epoch [9/10], Step [12400/12500], Loss: 0.0661\n",
            "Epoch [10/10], Step [200/12500], Loss: 0.0384\n",
            "Epoch [10/10], Step [400/12500], Loss: 0.0465\n",
            "Epoch [10/10], Step [600/12500], Loss: 0.0898\n",
            "Epoch [10/10], Step [800/12500], Loss: 0.0242\n",
            "Epoch [10/10], Step [1000/12500], Loss: 0.0577\n",
            "Epoch [10/10], Step [1200/12500], Loss: 0.0531\n",
            "Epoch [10/10], Step [1400/12500], Loss: 0.0558\n",
            "Epoch [10/10], Step [1600/12500], Loss: 0.0404\n",
            "Epoch [10/10], Step [1800/12500], Loss: 0.0892\n",
            "Epoch [10/10], Step [2000/12500], Loss: 0.0722\n",
            "Epoch [10/10], Step [2200/12500], Loss: 0.0474\n",
            "Epoch [10/10], Step [2400/12500], Loss: 0.0303\n",
            "Epoch [10/10], Step [2600/12500], Loss: 0.0444\n",
            "Epoch [10/10], Step [2800/12500], Loss: 0.0349\n",
            "Epoch [10/10], Step [3000/12500], Loss: 0.0533\n",
            "Epoch [10/10], Step [3200/12500], Loss: 0.0282\n",
            "Epoch [10/10], Step [3400/12500], Loss: 0.0272\n",
            "Epoch [10/10], Step [3600/12500], Loss: 0.0396\n",
            "Epoch [10/10], Step [3800/12500], Loss: 0.0455\n",
            "Epoch [10/10], Step [4000/12500], Loss: 0.0452\n",
            "Epoch [10/10], Step [4200/12500], Loss: 0.0418\n",
            "Epoch [10/10], Step [4400/12500], Loss: 0.0383\n",
            "Epoch [10/10], Step [4600/12500], Loss: 0.0514\n",
            "Epoch [10/10], Step [4800/12500], Loss: 0.0708\n",
            "Epoch [10/10], Step [5000/12500], Loss: 0.0447\n",
            "Epoch [10/10], Step [5200/12500], Loss: 0.0655\n",
            "Epoch [10/10], Step [5400/12500], Loss: 0.0412\n",
            "Epoch [10/10], Step [5600/12500], Loss: 0.0252\n",
            "Epoch [10/10], Step [5800/12500], Loss: 0.0418\n",
            "Epoch [10/10], Step [6000/12500], Loss: 0.0429\n",
            "Epoch [10/10], Step [6200/12500], Loss: 0.0711\n",
            "Epoch [10/10], Step [6400/12500], Loss: 0.0503\n",
            "Epoch [10/10], Step [6600/12500], Loss: 0.0672\n",
            "Epoch [10/10], Step [6800/12500], Loss: 0.0512\n",
            "Epoch [10/10], Step [7000/12500], Loss: 0.0354\n",
            "Epoch [10/10], Step [7200/12500], Loss: 0.0600\n",
            "Epoch [10/10], Step [7400/12500], Loss: 0.0589\n",
            "Epoch [10/10], Step [7600/12500], Loss: 0.0963\n",
            "Epoch [10/10], Step [7800/12500], Loss: 0.0883\n",
            "Epoch [10/10], Step [8000/12500], Loss: 0.0643\n",
            "Epoch [10/10], Step [8200/12500], Loss: 0.0522\n",
            "Epoch [10/10], Step [8400/12500], Loss: 0.0600\n",
            "Epoch [10/10], Step [8600/12500], Loss: 0.0595\n",
            "Epoch [10/10], Step [8800/12500], Loss: 0.0577\n",
            "Epoch [10/10], Step [9000/12500], Loss: 0.0804\n",
            "Epoch [10/10], Step [9200/12500], Loss: 0.0548\n",
            "Epoch [10/10], Step [9400/12500], Loss: 0.0493\n",
            "Epoch [10/10], Step [9600/12500], Loss: 0.0596\n",
            "Epoch [10/10], Step [9800/12500], Loss: 0.0893\n",
            "Epoch [10/10], Step [10000/12500], Loss: 0.0770\n",
            "Epoch [10/10], Step [10200/12500], Loss: 0.0641\n",
            "Epoch [10/10], Step [10400/12500], Loss: 0.0581\n",
            "Epoch [10/10], Step [10600/12500], Loss: 0.0373\n",
            "Epoch [10/10], Step [10800/12500], Loss: 0.0727\n",
            "Epoch [10/10], Step [11000/12500], Loss: 0.0532\n",
            "Epoch [10/10], Step [11200/12500], Loss: 0.0602\n",
            "Epoch [10/10], Step [11400/12500], Loss: 0.0720\n",
            "Epoch [10/10], Step [11600/12500], Loss: 0.0285\n",
            "Epoch [10/10], Step [11800/12500], Loss: 0.0481\n",
            "Epoch [10/10], Step [12000/12500], Loss: 0.0548\n",
            "Epoch [10/10], Step [12200/12500], Loss: 0.0670\n",
            "Epoch [10/10], Step [12400/12500], Loss: 0.0680\n",
            "Finished training\n",
            "Accuracy of the network on the 10000 test images: 74 %\n",
            "Freezing all layers and randomising fc3\n",
            "Saved frozen model with unfrozen fc3. Resetted fc3 parameters and stored in 'base.pt.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYCMQAGWIHE6",
        "outputId": "08cfe34a-cb1d-4d4d-f828-ec2e3d87c519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['references.bib',\n",
              " 'additional files',\n",
              " 'loss_function.py',\n",
              " 'base.pt',\n",
              " 'sgd.pt',\n",
              " 'main.py',\n",
              " 'sgd.py',\n",
              " '__pycache__',\n",
              " 'cso.py',\n",
              " 'requirements_gpu.txt',\n",
              " 'data',\n",
              " 'visualisation.py',\n",
              " 'Makefile',\n",
              " 'requirements.txt',\n",
              " 'nsgaII.py',\n",
              " '.gitignore',\n",
              " 'readme.md',\n",
              " '.git',\n",
              " 'sqn.py',\n",
              " 'de.py',\n",
              " 'report.tex']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cso.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9Ar3kNyISWY",
        "outputId": "fbc91cce-003a-4ea2-f3d3-99cbcb0ad3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/cso.py\", line 192, in <module>\n",
            "    population_size=POPULATION_SIZE,\n",
            "                    ^^^^^^^^^^^^^^^\n",
            "NameError: name 'POPULATION_SIZE' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sgd.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uanKfdvqJ4yq",
        "outputId": "79db979a-00e8-497e-f0a7-92f1be1aa98a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Starting training\n",
            "\n",
            "Epoch [1/10], Step [200/12500], Loss: 2.2947\n",
            "Epoch [1/10], Step [400/12500], Loss: 2.2725\n",
            "Epoch [1/10], Step [600/12500], Loss: 2.1912\n",
            "Epoch [1/10], Step [800/12500], Loss: 2.0563\n",
            "Epoch [1/10], Step [1000/12500], Loss: 1.9824\n",
            "Epoch [1/10], Step [1200/12500], Loss: 1.9454\n",
            "Epoch [1/10], Step [1400/12500], Loss: 1.9190\n",
            "Epoch [1/10], Step [1600/12500], Loss: 1.8647\n",
            "Epoch [1/10], Step [1800/12500], Loss: 1.8184\n",
            "Epoch [1/10], Step [2000/12500], Loss: 1.8190\n",
            "Epoch [1/10], Step [2200/12500], Loss: 1.7283\n",
            "Epoch [1/10], Step [2400/12500], Loss: 1.7119\n",
            "Epoch [1/10], Step [2600/12500], Loss: 1.6551\n",
            "Epoch [1/10], Step [2800/12500], Loss: 1.6226\n",
            "Epoch [1/10], Step [3000/12500], Loss: 1.6970\n",
            "Epoch [1/10], Step [3200/12500], Loss: 1.6145\n",
            "Epoch [1/10], Step [3400/12500], Loss: 1.5404\n",
            "Epoch [1/10], Step [3600/12500], Loss: 1.5335\n",
            "Epoch [1/10], Step [3800/12500], Loss: 1.5084\n",
            "Epoch [1/10], Step [4000/12500], Loss: 1.5761\n",
            "Epoch [1/10], Step [4200/12500], Loss: 1.5467\n",
            "Epoch [1/10], Step [4400/12500], Loss: 1.5352\n",
            "Epoch [1/10], Step [4600/12500], Loss: 1.4394\n",
            "Epoch [1/10], Step [4800/12500], Loss: 1.4595\n",
            "Epoch [1/10], Step [5000/12500], Loss: 1.4745\n",
            "Epoch [1/10], Step [5200/12500], Loss: 1.3867\n",
            "Epoch [1/10], Step [5400/12500], Loss: 1.4232\n",
            "Epoch [1/10], Step [5600/12500], Loss: 1.3889\n",
            "Epoch [1/10], Step [5800/12500], Loss: 1.4090\n",
            "Epoch [1/10], Step [6000/12500], Loss: 1.3763\n",
            "Epoch [1/10], Step [6200/12500], Loss: 1.3744\n",
            "Epoch [1/10], Step [6400/12500], Loss: 1.2722\n",
            "Epoch [1/10], Step [6600/12500], Loss: 1.3177\n",
            "Epoch [1/10], Step [6800/12500], Loss: 1.3301\n",
            "Epoch [1/10], Step [7000/12500], Loss: 1.3362\n",
            "Epoch [1/10], Step [7200/12500], Loss: 1.3443\n",
            "Epoch [1/10], Step [7400/12500], Loss: 1.3123\n",
            "Epoch [1/10], Step [7600/12500], Loss: 1.3510\n",
            "Epoch [1/10], Step [7800/12500], Loss: 1.3012\n",
            "Epoch [1/10], Step [8000/12500], Loss: 1.3060\n",
            "Epoch [1/10], Step [8200/12500], Loss: 1.2008\n",
            "Epoch [1/10], Step [8400/12500], Loss: 1.2371\n",
            "Epoch [1/10], Step [8600/12500], Loss: 1.3163\n",
            "Epoch [1/10], Step [8800/12500], Loss: 1.2609\n",
            "Epoch [1/10], Step [9000/12500], Loss: 1.2686\n",
            "Epoch [1/10], Step [9200/12500], Loss: 1.2340\n",
            "Epoch [1/10], Step [9400/12500], Loss: 1.1912\n",
            "Epoch [1/10], Step [9600/12500], Loss: 1.2033\n",
            "Epoch [1/10], Step [9800/12500], Loss: 1.1848\n",
            "Epoch [1/10], Step [10000/12500], Loss: 1.2629\n",
            "Epoch [1/10], Step [10200/12500], Loss: 1.1466\n",
            "Epoch [1/10], Step [10400/12500], Loss: 1.1281\n",
            "Epoch [1/10], Step [10600/12500], Loss: 1.1619\n",
            "Epoch [1/10], Step [10800/12500], Loss: 1.2213\n",
            "Epoch [1/10], Step [11000/12500], Loss: 1.1571\n",
            "Epoch [1/10], Step [11200/12500], Loss: 1.2075\n",
            "Epoch [1/10], Step [11400/12500], Loss: 1.2184\n",
            "Epoch [1/10], Step [11600/12500], Loss: 1.1475\n",
            "Epoch [1/10], Step [11800/12500], Loss: 1.1486\n",
            "Epoch [1/10], Step [12000/12500], Loss: 1.0948\n",
            "Epoch [1/10], Step [12200/12500], Loss: 1.1478\n",
            "Epoch [1/10], Step [12400/12500], Loss: 1.1181\n",
            "Epoch [2/10], Step [200/12500], Loss: 1.0519\n",
            "Epoch [2/10], Step [400/12500], Loss: 1.0776\n",
            "Epoch [2/10], Step [600/12500], Loss: 1.0857\n",
            "Epoch [2/10], Step [800/12500], Loss: 1.0730\n",
            "Epoch [2/10], Step [1000/12500], Loss: 1.0957\n",
            "Epoch [2/10], Step [1200/12500], Loss: 1.0932\n",
            "Epoch [2/10], Step [1400/12500], Loss: 1.0766\n",
            "Epoch [2/10], Step [1600/12500], Loss: 1.0006\n",
            "Epoch [2/10], Step [1800/12500], Loss: 1.0493\n",
            "Epoch [2/10], Step [2000/12500], Loss: 1.1308\n",
            "Epoch [2/10], Step [2200/12500], Loss: 1.0500\n",
            "Epoch [2/10], Step [2400/12500], Loss: 0.9658\n",
            "Epoch [2/10], Step [2600/12500], Loss: 1.0163\n",
            "Epoch [2/10], Step [2800/12500], Loss: 0.9751\n",
            "Epoch [2/10], Step [3000/12500], Loss: 1.0065\n",
            "Epoch [2/10], Step [3200/12500], Loss: 1.0590\n",
            "Epoch [2/10], Step [3400/12500], Loss: 1.0247\n",
            "Epoch [2/10], Step [3600/12500], Loss: 0.9905\n",
            "Epoch [2/10], Step [3800/12500], Loss: 0.9814\n",
            "Epoch [2/10], Step [4000/12500], Loss: 0.9904\n",
            "Epoch [2/10], Step [4200/12500], Loss: 1.0188\n",
            "Epoch [2/10], Step [4400/12500], Loss: 1.0256\n",
            "Epoch [2/10], Step [4600/12500], Loss: 0.9643\n",
            "Epoch [2/10], Step [4800/12500], Loss: 0.9525\n",
            "Epoch [2/10], Step [5000/12500], Loss: 0.9841\n",
            "Epoch [2/10], Step [5200/12500], Loss: 0.9649\n",
            "Epoch [2/10], Step [5400/12500], Loss: 0.9490\n",
            "Epoch [2/10], Step [5600/12500], Loss: 1.0032\n",
            "Epoch [2/10], Step [5800/12500], Loss: 1.0264\n",
            "Epoch [2/10], Step [6000/12500], Loss: 0.9404\n",
            "Epoch [2/10], Step [6200/12500], Loss: 0.9581\n",
            "Epoch [2/10], Step [6400/12500], Loss: 0.9630\n",
            "Epoch [2/10], Step [6600/12500], Loss: 0.9500\n",
            "Epoch [2/10], Step [6800/12500], Loss: 0.9542\n",
            "Epoch [2/10], Step [7000/12500], Loss: 0.9403\n",
            "Epoch [2/10], Step [7200/12500], Loss: 0.9259\n",
            "Epoch [2/10], Step [7400/12500], Loss: 0.8916\n",
            "Epoch [2/10], Step [7600/12500], Loss: 0.9317\n",
            "Epoch [2/10], Step [7800/12500], Loss: 0.9970\n",
            "Epoch [2/10], Step [8000/12500], Loss: 0.8872\n",
            "Epoch [2/10], Step [8200/12500], Loss: 0.9185\n",
            "Epoch [2/10], Step [8400/12500], Loss: 0.9121\n",
            "Epoch [2/10], Step [8600/12500], Loss: 0.8553\n",
            "Epoch [2/10], Step [8800/12500], Loss: 0.9223\n",
            "Epoch [2/10], Step [9000/12500], Loss: 0.8841\n",
            "Epoch [2/10], Step [9200/12500], Loss: 0.8466\n",
            "Epoch [2/10], Step [9400/12500], Loss: 0.8495\n",
            "Epoch [2/10], Step [9600/12500], Loss: 0.9261\n",
            "Epoch [2/10], Step [9800/12500], Loss: 0.8679\n",
            "Epoch [2/10], Step [10000/12500], Loss: 0.9533\n",
            "Epoch [2/10], Step [10200/12500], Loss: 0.9212\n",
            "Epoch [2/10], Step [10400/12500], Loss: 0.9537\n",
            "Epoch [2/10], Step [10600/12500], Loss: 0.8671\n",
            "Epoch [2/10], Step [10800/12500], Loss: 0.8977\n",
            "Epoch [2/10], Step [11000/12500], Loss: 0.8941\n",
            "Epoch [2/10], Step [11200/12500], Loss: 0.8566\n",
            "Epoch [2/10], Step [11400/12500], Loss: 0.9196\n",
            "Epoch [2/10], Step [11600/12500], Loss: 0.8468\n",
            "Epoch [2/10], Step [11800/12500], Loss: 0.8591\n",
            "Epoch [2/10], Step [12000/12500], Loss: 0.8795\n",
            "Epoch [2/10], Step [12200/12500], Loss: 0.8449\n",
            "Epoch [2/10], Step [12400/12500], Loss: 0.8690\n",
            "Epoch [3/10], Step [200/12500], Loss: 0.7459\n",
            "Epoch [3/10], Step [400/12500], Loss: 0.7092\n",
            "Epoch [3/10], Step [600/12500], Loss: 0.7724\n",
            "Epoch [3/10], Step [800/12500], Loss: 0.8016\n",
            "Epoch [3/10], Step [1000/12500], Loss: 0.7300\n",
            "Epoch [3/10], Step [1200/12500], Loss: 0.6465\n",
            "Epoch [3/10], Step [1400/12500], Loss: 0.7449\n",
            "Epoch [3/10], Step [1600/12500], Loss: 0.7852\n",
            "Epoch [3/10], Step [1800/12500], Loss: 0.6676\n",
            "Epoch [3/10], Step [2000/12500], Loss: 0.7236\n",
            "Epoch [3/10], Step [2200/12500], Loss: 0.7651\n",
            "Epoch [3/10], Step [2400/12500], Loss: 0.7386\n",
            "Epoch [3/10], Step [2600/12500], Loss: 0.7519\n",
            "Epoch [3/10], Step [2800/12500], Loss: 0.7383\n",
            "Epoch [3/10], Step [3000/12500], Loss: 0.7027\n",
            "Epoch [3/10], Step [3200/12500], Loss: 0.7251\n",
            "Epoch [3/10], Step [3400/12500], Loss: 0.7605\n",
            "Epoch [3/10], Step [3600/12500], Loss: 0.6958\n",
            "Epoch [3/10], Step [3800/12500], Loss: 0.7206\n",
            "Epoch [3/10], Step [4000/12500], Loss: 0.7343\n",
            "Epoch [3/10], Step [4200/12500], Loss: 0.6889\n",
            "Epoch [3/10], Step [4400/12500], Loss: 0.7760\n",
            "Epoch [3/10], Step [4600/12500], Loss: 0.7234\n",
            "Epoch [3/10], Step [4800/12500], Loss: 0.7004\n",
            "Epoch [3/10], Step [5000/12500], Loss: 0.6498\n",
            "Epoch [3/10], Step [5200/12500], Loss: 0.7148\n",
            "Epoch [3/10], Step [5400/12500], Loss: 0.7798\n",
            "Epoch [3/10], Step [5600/12500], Loss: 0.7082\n",
            "Epoch [3/10], Step [5800/12500], Loss: 0.7175\n",
            "Epoch [3/10], Step [6000/12500], Loss: 0.7165\n",
            "Epoch [3/10], Step [6200/12500], Loss: 0.7320\n",
            "Epoch [3/10], Step [6400/12500], Loss: 0.7712\n",
            "Epoch [3/10], Step [6600/12500], Loss: 0.7131\n",
            "Epoch [3/10], Step [6800/12500], Loss: 0.7217\n",
            "Epoch [3/10], Step [7000/12500], Loss: 0.7242\n",
            "Epoch [3/10], Step [7200/12500], Loss: 0.6593\n",
            "Epoch [3/10], Step [7400/12500], Loss: 0.6947\n",
            "Epoch [3/10], Step [7600/12500], Loss: 0.7211\n",
            "Epoch [3/10], Step [7800/12500], Loss: 0.7212\n",
            "Epoch [3/10], Step [8000/12500], Loss: 0.7780\n",
            "Epoch [3/10], Step [8200/12500], Loss: 0.6707\n",
            "Epoch [3/10], Step [8400/12500], Loss: 0.6889\n",
            "Epoch [3/10], Step [8600/12500], Loss: 0.7332\n",
            "Epoch [3/10], Step [8800/12500], Loss: 0.6754\n",
            "Epoch [3/10], Step [9000/12500], Loss: 0.7524\n",
            "Epoch [3/10], Step [9200/12500], Loss: 0.7307\n",
            "Epoch [3/10], Step [9400/12500], Loss: 0.7248\n",
            "Epoch [3/10], Step [9600/12500], Loss: 0.7250\n",
            "Epoch [3/10], Step [9800/12500], Loss: 0.6859\n",
            "Epoch [3/10], Step [10000/12500], Loss: 0.7206\n",
            "Epoch [3/10], Step [10200/12500], Loss: 0.7097\n",
            "Epoch [3/10], Step [10400/12500], Loss: 0.7191\n",
            "Epoch [3/10], Step [10600/12500], Loss: 0.7571\n",
            "Epoch [3/10], Step [10800/12500], Loss: 0.7050\n",
            "Epoch [3/10], Step [11000/12500], Loss: 0.7116\n",
            "Epoch [3/10], Step [11200/12500], Loss: 0.7249\n",
            "Epoch [3/10], Step [11400/12500], Loss: 0.6654\n",
            "Epoch [3/10], Step [11600/12500], Loss: 0.6894\n",
            "Epoch [3/10], Step [11800/12500], Loss: 0.7045\n",
            "Epoch [3/10], Step [12000/12500], Loss: 0.6304\n",
            "Epoch [3/10], Step [12200/12500], Loss: 0.6961\n",
            "Epoch [3/10], Step [12400/12500], Loss: 0.7444\n",
            "Epoch [4/10], Step [200/12500], Loss: 0.5085\n",
            "Epoch [4/10], Step [400/12500], Loss: 0.4746\n",
            "Epoch [4/10], Step [600/12500], Loss: 0.5205\n",
            "Epoch [4/10], Step [800/12500], Loss: 0.4988\n",
            "Epoch [4/10], Step [1000/12500], Loss: 0.5035\n",
            "Epoch [4/10], Step [1200/12500], Loss: 0.5256\n",
            "Epoch [4/10], Step [1400/12500], Loss: 0.4564\n",
            "Epoch [4/10], Step [1600/12500], Loss: 0.5089\n",
            "Epoch [4/10], Step [1800/12500], Loss: 0.4567\n",
            "Epoch [4/10], Step [2000/12500], Loss: 0.5002\n",
            "Epoch [4/10], Step [2200/12500], Loss: 0.5043\n",
            "Epoch [4/10], Step [2400/12500], Loss: 0.4585\n",
            "Epoch [4/10], Step [2600/12500], Loss: 0.5508\n",
            "Epoch [4/10], Step [2800/12500], Loss: 0.5260\n",
            "Epoch [4/10], Step [3000/12500], Loss: 0.5231\n",
            "Epoch [4/10], Step [3200/12500], Loss: 0.4445\n",
            "Epoch [4/10], Step [3400/12500], Loss: 0.4542\n",
            "Epoch [4/10], Step [3600/12500], Loss: 0.5116\n",
            "Epoch [4/10], Step [3800/12500], Loss: 0.4626\n",
            "Epoch [4/10], Step [4000/12500], Loss: 0.5325\n",
            "Epoch [4/10], Step [4200/12500], Loss: 0.4974\n",
            "Epoch [4/10], Step [4400/12500], Loss: 0.5405\n",
            "Epoch [4/10], Step [4600/12500], Loss: 0.5344\n",
            "Epoch [4/10], Step [4800/12500], Loss: 0.5278\n",
            "Epoch [4/10], Step [5000/12500], Loss: 0.4837\n",
            "Epoch [4/10], Step [5200/12500], Loss: 0.5167\n",
            "Epoch [4/10], Step [5400/12500], Loss: 0.4937\n",
            "Epoch [4/10], Step [5600/12500], Loss: 0.5012\n",
            "Epoch [4/10], Step [5800/12500], Loss: 0.5019\n",
            "Epoch [4/10], Step [6000/12500], Loss: 0.5554\n",
            "Epoch [4/10], Step [6200/12500], Loss: 0.5728\n",
            "Epoch [4/10], Step [6400/12500], Loss: 0.5061\n",
            "Epoch [4/10], Step [6600/12500], Loss: 0.5544\n",
            "Epoch [4/10], Step [6800/12500], Loss: 0.5153\n",
            "Epoch [4/10], Step [7000/12500], Loss: 0.4608\n",
            "Epoch [4/10], Step [7200/12500], Loss: 0.5359\n",
            "Epoch [4/10], Step [7400/12500], Loss: 0.5331\n",
            "Epoch [4/10], Step [7600/12500], Loss: 0.5038\n",
            "Epoch [4/10], Step [7800/12500], Loss: 0.5173\n",
            "Epoch [4/10], Step [8000/12500], Loss: 0.5205\n",
            "Epoch [4/10], Step [8200/12500], Loss: 0.5091\n",
            "Epoch [4/10], Step [8400/12500], Loss: 0.4756\n",
            "Epoch [4/10], Step [8600/12500], Loss: 0.5422\n",
            "Epoch [4/10], Step [8800/12500], Loss: 0.5704\n",
            "Epoch [4/10], Step [9000/12500], Loss: 0.5463\n",
            "Epoch [4/10], Step [9200/12500], Loss: 0.5560\n",
            "Epoch [4/10], Step [9400/12500], Loss: 0.4977\n",
            "Epoch [4/10], Step [9600/12500], Loss: 0.5704\n",
            "Epoch [4/10], Step [9800/12500], Loss: 0.4695\n",
            "Epoch [4/10], Step [10000/12500], Loss: 0.5476\n",
            "Epoch [4/10], Step [10200/12500], Loss: 0.5066\n",
            "Epoch [4/10], Step [10400/12500], Loss: 0.5081\n",
            "Epoch [4/10], Step [10600/12500], Loss: 0.6302\n",
            "Epoch [4/10], Step [10800/12500], Loss: 0.5106\n",
            "Epoch [4/10], Step [11000/12500], Loss: 0.5337\n",
            "Epoch [4/10], Step [11200/12500], Loss: 0.5196\n",
            "Epoch [4/10], Step [11400/12500], Loss: 0.4982\n",
            "Epoch [4/10], Step [11600/12500], Loss: 0.5177\n",
            "Epoch [4/10], Step [11800/12500], Loss: 0.5826\n",
            "Epoch [4/10], Step [12000/12500], Loss: 0.5162\n",
            "Epoch [4/10], Step [12200/12500], Loss: 0.5503\n",
            "Epoch [4/10], Step [12400/12500], Loss: 0.5273\n",
            "Epoch [5/10], Step [200/12500], Loss: 0.3209\n",
            "Epoch [5/10], Step [400/12500], Loss: 0.2924\n",
            "Epoch [5/10], Step [600/12500], Loss: 0.2998\n",
            "Epoch [5/10], Step [800/12500], Loss: 0.2724\n",
            "Epoch [5/10], Step [1000/12500], Loss: 0.3010\n",
            "Epoch [5/10], Step [1200/12500], Loss: 0.2418\n",
            "Epoch [5/10], Step [1400/12500], Loss: 0.2578\n",
            "Epoch [5/10], Step [1600/12500], Loss: 0.3094\n",
            "Epoch [5/10], Step [1800/12500], Loss: 0.2972\n",
            "Epoch [5/10], Step [2000/12500], Loss: 0.2835\n",
            "Epoch [5/10], Step [2200/12500], Loss: 0.3070\n",
            "Epoch [5/10], Step [2400/12500], Loss: 0.2800\n",
            "Epoch [5/10], Step [2600/12500], Loss: 0.3413\n",
            "Epoch [5/10], Step [2800/12500], Loss: 0.2615\n",
            "Epoch [5/10], Step [3000/12500], Loss: 0.3329\n",
            "Epoch [5/10], Step [3200/12500], Loss: 0.3465\n",
            "Epoch [5/10], Step [3400/12500], Loss: 0.3029\n",
            "Epoch [5/10], Step [3600/12500], Loss: 0.3464\n",
            "Epoch [5/10], Step [3800/12500], Loss: 0.2986\n",
            "Epoch [5/10], Step [4000/12500], Loss: 0.3490\n",
            "Epoch [5/10], Step [4200/12500], Loss: 0.3032\n",
            "Epoch [5/10], Step [4400/12500], Loss: 0.3258\n",
            "Epoch [5/10], Step [4600/12500], Loss: 0.3384\n",
            "Epoch [5/10], Step [4800/12500], Loss: 0.3395\n",
            "Epoch [5/10], Step [5000/12500], Loss: 0.2887\n",
            "Epoch [5/10], Step [5200/12500], Loss: 0.2746\n",
            "Epoch [5/10], Step [5400/12500], Loss: 0.2944\n",
            "Epoch [5/10], Step [5600/12500], Loss: 0.3392\n",
            "Epoch [5/10], Step [5800/12500], Loss: 0.3408\n",
            "Epoch [5/10], Step [6000/12500], Loss: 0.3435\n",
            "Epoch [5/10], Step [6200/12500], Loss: 0.3574\n",
            "Epoch [5/10], Step [6400/12500], Loss: 0.3052\n",
            "Epoch [5/10], Step [6600/12500], Loss: 0.3597\n",
            "Epoch [5/10], Step [6800/12500], Loss: 0.3495\n",
            "Epoch [5/10], Step [7000/12500], Loss: 0.2390\n",
            "Epoch [5/10], Step [7200/12500], Loss: 0.3148\n",
            "Epoch [5/10], Step [7400/12500], Loss: 0.3486\n",
            "Epoch [5/10], Step [7600/12500], Loss: 0.3225\n",
            "Epoch [5/10], Step [7800/12500], Loss: 0.3939\n",
            "Epoch [5/10], Step [8000/12500], Loss: 0.4077\n",
            "Epoch [5/10], Step [8200/12500], Loss: 0.4829\n",
            "Epoch [5/10], Step [8400/12500], Loss: 0.3849\n",
            "Epoch [5/10], Step [8600/12500], Loss: 0.3171\n",
            "Epoch [5/10], Step [8800/12500], Loss: 0.2934\n",
            "Epoch [5/10], Step [9000/12500], Loss: 0.3139\n",
            "Epoch [5/10], Step [9200/12500], Loss: 0.3479\n",
            "Epoch [5/10], Step [9400/12500], Loss: 0.3830\n",
            "Epoch [5/10], Step [9600/12500], Loss: 0.3694\n",
            "Epoch [5/10], Step [9800/12500], Loss: 0.3655\n",
            "Epoch [5/10], Step [10000/12500], Loss: 0.3474\n",
            "Epoch [5/10], Step [10200/12500], Loss: 0.3567\n",
            "Epoch [5/10], Step [10400/12500], Loss: 0.3391\n",
            "Epoch [5/10], Step [10600/12500], Loss: 0.3360\n",
            "Epoch [5/10], Step [10800/12500], Loss: 0.3395\n",
            "Epoch [5/10], Step [11000/12500], Loss: 0.3539\n",
            "Epoch [5/10], Step [11200/12500], Loss: 0.3751\n",
            "Epoch [5/10], Step [11400/12500], Loss: 0.3608\n",
            "Epoch [5/10], Step [11600/12500], Loss: 0.3538\n",
            "Epoch [5/10], Step [11800/12500], Loss: 0.3097\n",
            "Epoch [5/10], Step [12000/12500], Loss: 0.3940\n",
            "Epoch [5/10], Step [12200/12500], Loss: 0.3265\n",
            "Epoch [5/10], Step [12400/12500], Loss: 0.3797\n",
            "Epoch [6/10], Step [200/12500], Loss: 0.1537\n",
            "Epoch [6/10], Step [400/12500], Loss: 0.1625\n",
            "Epoch [6/10], Step [600/12500], Loss: 0.1389\n",
            "Epoch [6/10], Step [800/12500], Loss: 0.1273\n",
            "Epoch [6/10], Step [1000/12500], Loss: 0.1262\n",
            "Epoch [6/10], Step [1200/12500], Loss: 0.1372\n",
            "Epoch [6/10], Step [1400/12500], Loss: 0.1276\n",
            "Epoch [6/10], Step [1600/12500], Loss: 0.1301\n",
            "Epoch [6/10], Step [1800/12500], Loss: 0.1423\n",
            "Epoch [6/10], Step [2000/12500], Loss: 0.1381\n",
            "Epoch [6/10], Step [2200/12500], Loss: 0.1247\n",
            "Epoch [6/10], Step [2400/12500], Loss: 0.1494\n",
            "Epoch [6/10], Step [2600/12500], Loss: 0.1483\n",
            "Epoch [6/10], Step [2800/12500], Loss: 0.1870\n",
            "Epoch [6/10], Step [3000/12500], Loss: 0.1430\n",
            "Epoch [6/10], Step [3200/12500], Loss: 0.1869\n",
            "Epoch [6/10], Step [3400/12500], Loss: 0.2063\n",
            "Epoch [6/10], Step [3600/12500], Loss: 0.1737\n",
            "Epoch [6/10], Step [3800/12500], Loss: 0.2359\n",
            "Epoch [6/10], Step [4000/12500], Loss: 0.1783\n",
            "Epoch [6/10], Step [4200/12500], Loss: 0.1885\n",
            "Epoch [6/10], Step [4400/12500], Loss: 0.1448\n",
            "Epoch [6/10], Step [4600/12500], Loss: 0.1874\n",
            "Epoch [6/10], Step [4800/12500], Loss: 0.1875\n",
            "Epoch [6/10], Step [5000/12500], Loss: 0.1728\n",
            "Epoch [6/10], Step [5200/12500], Loss: 0.1791\n",
            "Epoch [6/10], Step [5400/12500], Loss: 0.1567\n",
            "Epoch [6/10], Step [5600/12500], Loss: 0.1872\n",
            "Epoch [6/10], Step [5800/12500], Loss: 0.1511\n",
            "Epoch [6/10], Step [6000/12500], Loss: 0.1973\n",
            "Epoch [6/10], Step [6200/12500], Loss: 0.1358\n",
            "Epoch [6/10], Step [6400/12500], Loss: 0.1553\n",
            "Epoch [6/10], Step [6600/12500], Loss: 0.2038\n",
            "Epoch [6/10], Step [6800/12500], Loss: 0.1733\n",
            "Epoch [6/10], Step [7000/12500], Loss: 0.1813\n",
            "Epoch [6/10], Step [7200/12500], Loss: 0.1790\n",
            "Epoch [6/10], Step [7400/12500], Loss: 0.2051\n",
            "Epoch [6/10], Step [7600/12500], Loss: 0.1505\n",
            "Epoch [6/10], Step [7800/12500], Loss: 0.2189\n",
            "Epoch [6/10], Step [8000/12500], Loss: 0.2053\n",
            "Epoch [6/10], Step [8200/12500], Loss: 0.2153\n",
            "Epoch [6/10], Step [8400/12500], Loss: 0.2116\n",
            "Epoch [6/10], Step [8600/12500], Loss: 0.1599\n",
            "Epoch [6/10], Step [8800/12500], Loss: 0.1896\n",
            "Epoch [6/10], Step [9000/12500], Loss: 0.2111\n",
            "Epoch [6/10], Step [9200/12500], Loss: 0.1992\n",
            "Epoch [6/10], Step [9400/12500], Loss: 0.1824\n",
            "Epoch [6/10], Step [9600/12500], Loss: 0.2182\n",
            "Epoch [6/10], Step [9800/12500], Loss: 0.2210\n",
            "Epoch [6/10], Step [10000/12500], Loss: 0.2109\n",
            "Epoch [6/10], Step [10200/12500], Loss: 0.2829\n",
            "Epoch [6/10], Step [10400/12500], Loss: 0.2208\n",
            "Epoch [6/10], Step [10600/12500], Loss: 0.2573\n",
            "Epoch [6/10], Step [10800/12500], Loss: 0.2287\n",
            "Epoch [6/10], Step [11000/12500], Loss: 0.2198\n",
            "Epoch [6/10], Step [11200/12500], Loss: 0.2290\n",
            "Epoch [6/10], Step [11400/12500], Loss: 0.2453\n",
            "Epoch [6/10], Step [11600/12500], Loss: 0.2111\n",
            "Epoch [6/10], Step [11800/12500], Loss: 0.2221\n",
            "Epoch [6/10], Step [12000/12500], Loss: 0.2555\n",
            "Epoch [6/10], Step [12200/12500], Loss: 0.2131\n",
            "Epoch [6/10], Step [12400/12500], Loss: 0.2088\n",
            "Epoch [7/10], Step [200/12500], Loss: 0.0930\n",
            "Epoch [7/10], Step [400/12500], Loss: 0.1016\n",
            "Epoch [7/10], Step [600/12500], Loss: 0.0845\n",
            "Epoch [7/10], Step [800/12500], Loss: 0.0651\n",
            "Epoch [7/10], Step [1000/12500], Loss: 0.0738\n",
            "Epoch [7/10], Step [1200/12500], Loss: 0.0955\n",
            "Epoch [7/10], Step [1400/12500], Loss: 0.1215\n",
            "Epoch [7/10], Step [1600/12500], Loss: 0.1069\n",
            "Epoch [7/10], Step [1800/12500], Loss: 0.1065\n",
            "Epoch [7/10], Step [2000/12500], Loss: 0.1019\n",
            "Epoch [7/10], Step [2200/12500], Loss: 0.0846\n",
            "Epoch [7/10], Step [2400/12500], Loss: 0.0835\n",
            "Epoch [7/10], Step [2600/12500], Loss: 0.0849\n",
            "Epoch [7/10], Step [2800/12500], Loss: 0.1254\n",
            "Epoch [7/10], Step [3000/12500], Loss: 0.1038\n",
            "Epoch [7/10], Step [3200/12500], Loss: 0.0769\n",
            "Epoch [7/10], Step [3400/12500], Loss: 0.0678\n",
            "Epoch [7/10], Step [3600/12500], Loss: 0.1184\n",
            "Epoch [7/10], Step [3800/12500], Loss: 0.0872\n",
            "Epoch [7/10], Step [4000/12500], Loss: 0.0997\n",
            "Epoch [7/10], Step [4200/12500], Loss: 0.0753\n",
            "Epoch [7/10], Step [4400/12500], Loss: 0.0794\n",
            "Epoch [7/10], Step [4600/12500], Loss: 0.0924\n",
            "Epoch [7/10], Step [4800/12500], Loss: 0.0830\n",
            "Epoch [7/10], Step [5000/12500], Loss: 0.0713\n",
            "Epoch [7/10], Step [5200/12500], Loss: 0.0941\n",
            "Epoch [7/10], Step [5400/12500], Loss: 0.1351\n",
            "Epoch [7/10], Step [5600/12500], Loss: 0.1059\n",
            "Epoch [7/10], Step [5800/12500], Loss: 0.1259\n",
            "Epoch [7/10], Step [6000/12500], Loss: 0.0911\n",
            "Epoch [7/10], Step [6200/12500], Loss: 0.0743\n",
            "Epoch [7/10], Step [6400/12500], Loss: 0.1103\n",
            "Epoch [7/10], Step [6600/12500], Loss: 0.1523\n",
            "Epoch [7/10], Step [6800/12500], Loss: 0.1083\n",
            "Epoch [7/10], Step [7000/12500], Loss: 0.0753\n",
            "Epoch [7/10], Step [7200/12500], Loss: 0.1090\n",
            "Epoch [7/10], Step [7400/12500], Loss: 0.1317\n",
            "Epoch [7/10], Step [7600/12500], Loss: 0.1186\n",
            "Epoch [7/10], Step [7800/12500], Loss: 0.1312\n",
            "Epoch [7/10], Step [8000/12500], Loss: 0.1215\n",
            "Epoch [7/10], Step [8200/12500], Loss: 0.1153\n",
            "Epoch [7/10], Step [8400/12500], Loss: 0.1792\n",
            "Epoch [7/10], Step [8600/12500], Loss: 0.1350\n",
            "Epoch [7/10], Step [8800/12500], Loss: 0.1170\n",
            "Epoch [7/10], Step [9000/12500], Loss: 0.1446\n",
            "Epoch [7/10], Step [9200/12500], Loss: 0.1187\n",
            "Epoch [7/10], Step [9400/12500], Loss: 0.1427\n",
            "Epoch [7/10], Step [9600/12500], Loss: 0.1031\n",
            "Epoch [7/10], Step [9800/12500], Loss: 0.1566\n",
            "Epoch [7/10], Step [10000/12500], Loss: 0.1509\n",
            "Epoch [7/10], Step [10200/12500], Loss: 0.1072\n",
            "Epoch [7/10], Step [10400/12500], Loss: 0.1151\n",
            "Epoch [7/10], Step [10600/12500], Loss: 0.1469\n",
            "Epoch [7/10], Step [10800/12500], Loss: 0.1470\n",
            "Epoch [7/10], Step [11000/12500], Loss: 0.1513\n",
            "Epoch [7/10], Step [11200/12500], Loss: 0.0950\n",
            "Epoch [7/10], Step [11400/12500], Loss: 0.1502\n",
            "Epoch [7/10], Step [11600/12500], Loss: 0.1151\n",
            "Epoch [7/10], Step [11800/12500], Loss: 0.1185\n",
            "Epoch [7/10], Step [12000/12500], Loss: 0.1925\n",
            "Epoch [7/10], Step [12200/12500], Loss: 0.1727\n",
            "Epoch [7/10], Step [12400/12500], Loss: 0.1170\n",
            "Epoch [8/10], Step [200/12500], Loss: 0.0578\n",
            "Epoch [8/10], Step [400/12500], Loss: 0.1033\n",
            "Epoch [8/10], Step [600/12500], Loss: 0.0601\n",
            "Epoch [8/10], Step [800/12500], Loss: 0.0496\n",
            "Epoch [8/10], Step [1000/12500], Loss: 0.0683\n",
            "Epoch [8/10], Step [1200/12500], Loss: 0.0609\n",
            "Epoch [8/10], Step [1400/12500], Loss: 0.0555\n",
            "Epoch [8/10], Step [1600/12500], Loss: 0.0685\n",
            "Epoch [8/10], Step [1800/12500], Loss: 0.0394\n",
            "Epoch [8/10], Step [2000/12500], Loss: 0.0503\n",
            "Epoch [8/10], Step [2200/12500], Loss: 0.0817\n",
            "Epoch [8/10], Step [2400/12500], Loss: 0.0611\n",
            "Epoch [8/10], Step [2600/12500], Loss: 0.0842\n",
            "Epoch [8/10], Step [2800/12500], Loss: 0.1079\n",
            "Epoch [8/10], Step [3000/12500], Loss: 0.0527\n",
            "Epoch [8/10], Step [3200/12500], Loss: 0.0460\n",
            "Epoch [8/10], Step [3400/12500], Loss: 0.0617\n",
            "Epoch [8/10], Step [3600/12500], Loss: 0.0522\n",
            "Epoch [8/10], Step [3800/12500], Loss: 0.0773\n",
            "Epoch [8/10], Step [4000/12500], Loss: 0.0578\n",
            "Epoch [8/10], Step [4200/12500], Loss: 0.0773\n",
            "Epoch [8/10], Step [4400/12500], Loss: 0.1002\n",
            "Epoch [8/10], Step [4600/12500], Loss: 0.0750\n",
            "Epoch [8/10], Step [4800/12500], Loss: 0.0715\n",
            "Epoch [8/10], Step [5000/12500], Loss: 0.0621\n",
            "Epoch [8/10], Step [5200/12500], Loss: 0.1169\n",
            "Epoch [8/10], Step [5400/12500], Loss: 0.0849\n",
            "Epoch [8/10], Step [5600/12500], Loss: 0.0819\n",
            "Epoch [8/10], Step [5800/12500], Loss: 0.1099\n",
            "Epoch [8/10], Step [6000/12500], Loss: 0.1026\n",
            "Epoch [8/10], Step [6200/12500], Loss: 0.0900\n",
            "Epoch [8/10], Step [6400/12500], Loss: 0.0940\n",
            "Epoch [8/10], Step [6600/12500], Loss: 0.0695\n",
            "Epoch [8/10], Step [6800/12500], Loss: 0.0784\n",
            "Epoch [8/10], Step [7000/12500], Loss: 0.0730\n",
            "Epoch [8/10], Step [7200/12500], Loss: 0.0477\n",
            "Epoch [8/10], Step [7400/12500], Loss: 0.1246\n",
            "Epoch [8/10], Step [7600/12500], Loss: 0.0894\n",
            "Epoch [8/10], Step [7800/12500], Loss: 0.0847\n",
            "Epoch [8/10], Step [8000/12500], Loss: 0.0730\n",
            "Epoch [8/10], Step [8200/12500], Loss: 0.0419\n",
            "Epoch [8/10], Step [8400/12500], Loss: 0.1626\n",
            "Epoch [8/10], Step [8600/12500], Loss: 0.0899\n",
            "Epoch [8/10], Step [8800/12500], Loss: 0.0823\n",
            "Epoch [8/10], Step [9000/12500], Loss: 0.0508\n",
            "Epoch [8/10], Step [9200/12500], Loss: 0.0577\n",
            "Epoch [8/10], Step [9400/12500], Loss: 0.1201\n",
            "Epoch [8/10], Step [9600/12500], Loss: 0.1084\n",
            "Epoch [8/10], Step [9800/12500], Loss: 0.0944\n",
            "Epoch [8/10], Step [10000/12500], Loss: 0.1044\n",
            "Epoch [8/10], Step [10200/12500], Loss: 0.1034\n",
            "Epoch [8/10], Step [10400/12500], Loss: 0.1134\n",
            "Epoch [8/10], Step [10600/12500], Loss: 0.1230\n",
            "Epoch [8/10], Step [10800/12500], Loss: 0.1027\n",
            "Epoch [8/10], Step [11000/12500], Loss: 0.0867\n",
            "Epoch [8/10], Step [11200/12500], Loss: 0.1250\n",
            "Epoch [8/10], Step [11400/12500], Loss: 0.1008\n",
            "Epoch [8/10], Step [11600/12500], Loss: 0.1213\n",
            "Epoch [8/10], Step [11800/12500], Loss: 0.1173\n",
            "Epoch [8/10], Step [12000/12500], Loss: 0.1029\n",
            "Epoch [8/10], Step [12200/12500], Loss: 0.1646\n",
            "Epoch [8/10], Step [12400/12500], Loss: 0.1474\n",
            "Epoch [9/10], Step [200/12500], Loss: 0.0524\n",
            "Epoch [9/10], Step [400/12500], Loss: 0.0430\n",
            "Epoch [9/10], Step [600/12500], Loss: 0.0451\n",
            "Epoch [9/10], Step [800/12500], Loss: 0.0370\n",
            "Epoch [9/10], Step [1000/12500], Loss: 0.0426\n",
            "Epoch [9/10], Step [1200/12500], Loss: 0.0436\n",
            "Epoch [9/10], Step [1400/12500], Loss: 0.0468\n",
            "Epoch [9/10], Step [1600/12500], Loss: 0.0626\n",
            "Epoch [9/10], Step [1800/12500], Loss: 0.0660\n",
            "Epoch [9/10], Step [2000/12500], Loss: 0.0510\n",
            "Epoch [9/10], Step [2200/12500], Loss: 0.0642\n",
            "Epoch [9/10], Step [2400/12500], Loss: 0.0516\n",
            "Epoch [9/10], Step [2600/12500], Loss: 0.0367\n",
            "Epoch [9/10], Step [2800/12500], Loss: 0.0432\n",
            "Epoch [9/10], Step [3000/12500], Loss: 0.0355\n",
            "Epoch [9/10], Step [3200/12500], Loss: 0.0450\n",
            "Epoch [9/10], Step [3400/12500], Loss: 0.0373\n",
            "Epoch [9/10], Step [3600/12500], Loss: 0.0587\n",
            "Epoch [9/10], Step [3800/12500], Loss: 0.0645\n",
            "Epoch [9/10], Step [4000/12500], Loss: 0.0336\n",
            "Epoch [9/10], Step [4200/12500], Loss: 0.0299\n",
            "Epoch [9/10], Step [4400/12500], Loss: 0.0603\n",
            "Epoch [9/10], Step [4600/12500], Loss: 0.0265\n",
            "Epoch [9/10], Step [4800/12500], Loss: 0.0700\n",
            "Epoch [9/10], Step [5000/12500], Loss: 0.0519\n",
            "Epoch [9/10], Step [5200/12500], Loss: 0.0603\n",
            "Epoch [9/10], Step [5400/12500], Loss: 0.0792\n",
            "Epoch [9/10], Step [5600/12500], Loss: 0.0721\n",
            "Epoch [9/10], Step [5800/12500], Loss: 0.0658\n",
            "Epoch [9/10], Step [6000/12500], Loss: 0.0622\n",
            "Epoch [9/10], Step [6200/12500], Loss: 0.0740\n",
            "Epoch [9/10], Step [6400/12500], Loss: 0.0830\n",
            "Epoch [9/10], Step [6600/12500], Loss: 0.0923\n",
            "Epoch [9/10], Step [6800/12500], Loss: 0.0851\n",
            "Epoch [9/10], Step [7000/12500], Loss: 0.0970\n",
            "Epoch [9/10], Step [7200/12500], Loss: 0.0830\n",
            "Epoch [9/10], Step [7400/12500], Loss: 0.0817\n",
            "Epoch [9/10], Step [7600/12500], Loss: 0.0487\n",
            "Epoch [9/10], Step [7800/12500], Loss: 0.0503\n",
            "Epoch [9/10], Step [8000/12500], Loss: 0.1056\n",
            "Epoch [9/10], Step [8200/12500], Loss: 0.1005\n",
            "Epoch [9/10], Step [8400/12500], Loss: 0.0707\n",
            "Epoch [9/10], Step [8600/12500], Loss: 0.0653\n",
            "Epoch [9/10], Step [8800/12500], Loss: 0.0822\n",
            "Epoch [9/10], Step [9000/12500], Loss: 0.0773\n",
            "Epoch [9/10], Step [9200/12500], Loss: 0.0784\n",
            "Epoch [9/10], Step [9400/12500], Loss: 0.1157\n",
            "Epoch [9/10], Step [9600/12500], Loss: 0.1053\n",
            "Epoch [9/10], Step [9800/12500], Loss: 0.0683\n",
            "Epoch [9/10], Step [10000/12500], Loss: 0.0784\n",
            "Epoch [9/10], Step [10200/12500], Loss: 0.0705\n",
            "Epoch [9/10], Step [10400/12500], Loss: 0.0634\n",
            "Epoch [9/10], Step [10600/12500], Loss: 0.0766\n",
            "Epoch [9/10], Step [10800/12500], Loss: 0.0892\n",
            "Epoch [9/10], Step [11000/12500], Loss: 0.1183\n",
            "Epoch [9/10], Step [11200/12500], Loss: 0.1240\n",
            "Epoch [9/10], Step [11400/12500], Loss: 0.0838\n",
            "Epoch [9/10], Step [11600/12500], Loss: 0.0781\n",
            "Epoch [9/10], Step [11800/12500], Loss: 0.0983\n",
            "Epoch [9/10], Step [12000/12500], Loss: 0.1401\n",
            "Epoch [9/10], Step [12200/12500], Loss: 0.1200\n",
            "Epoch [9/10], Step [12400/12500], Loss: 0.1061\n",
            "Epoch [10/10], Step [200/12500], Loss: 0.0424\n",
            "Epoch [10/10], Step [400/12500], Loss: 0.0644\n",
            "Epoch [10/10], Step [600/12500], Loss: 0.0399\n",
            "Epoch [10/10], Step [800/12500], Loss: 0.0266\n",
            "Epoch [10/10], Step [1000/12500], Loss: 0.0863\n",
            "Epoch [10/10], Step [1200/12500], Loss: 0.0635\n",
            "Epoch [10/10], Step [1400/12500], Loss: 0.0283\n",
            "Epoch [10/10], Step [1600/12500], Loss: 0.0343\n",
            "Epoch [10/10], Step [1800/12500], Loss: 0.0531\n",
            "Epoch [10/10], Step [2000/12500], Loss: 0.0280\n",
            "Epoch [10/10], Step [2200/12500], Loss: 0.0628\n",
            "Epoch [10/10], Step [2400/12500], Loss: 0.0324\n",
            "Epoch [10/10], Step [2600/12500], Loss: 0.0336\n",
            "Epoch [10/10], Step [2800/12500], Loss: 0.0416\n",
            "Epoch [10/10], Step [3000/12500], Loss: 0.0330\n",
            "Epoch [10/10], Step [3200/12500], Loss: 0.0432\n",
            "Epoch [10/10], Step [3400/12500], Loss: 0.0659\n",
            "Epoch [10/10], Step [3600/12500], Loss: 0.0752\n",
            "Epoch [10/10], Step [3800/12500], Loss: 0.0517\n",
            "Epoch [10/10], Step [4000/12500], Loss: 0.0392\n",
            "Epoch [10/10], Step [4200/12500], Loss: 0.0680\n",
            "Epoch [10/10], Step [4400/12500], Loss: 0.0769\n",
            "Epoch [10/10], Step [4600/12500], Loss: 0.0393\n",
            "Epoch [10/10], Step [4800/12500], Loss: 0.0284\n",
            "Epoch [10/10], Step [5000/12500], Loss: 0.0339\n",
            "Epoch [10/10], Step [5200/12500], Loss: 0.0225\n",
            "Epoch [10/10], Step [5400/12500], Loss: 0.0473\n",
            "Epoch [10/10], Step [5600/12500], Loss: 0.0304\n",
            "Epoch [10/10], Step [5800/12500], Loss: 0.0626\n",
            "Epoch [10/10], Step [6000/12500], Loss: 0.0717\n",
            "Epoch [10/10], Step [6200/12500], Loss: 0.0476\n",
            "Epoch [10/10], Step [6400/12500], Loss: 0.0474\n",
            "Epoch [10/10], Step [6600/12500], Loss: 0.0660\n",
            "Epoch [10/10], Step [6800/12500], Loss: 0.0618\n",
            "Epoch [10/10], Step [7000/12500], Loss: 0.0481\n",
            "Epoch [10/10], Step [7200/12500], Loss: 0.0429\n",
            "Epoch [10/10], Step [7400/12500], Loss: 0.0319\n",
            "Epoch [10/10], Step [7600/12500], Loss: 0.0367\n",
            "Epoch [10/10], Step [7800/12500], Loss: 0.0558\n",
            "Epoch [10/10], Step [8000/12500], Loss: 0.0728\n",
            "Epoch [10/10], Step [8200/12500], Loss: 0.0407\n",
            "Epoch [10/10], Step [8400/12500], Loss: 0.0202\n",
            "Epoch [10/10], Step [8600/12500], Loss: 0.0576\n",
            "Epoch [10/10], Step [8800/12500], Loss: 0.0538\n",
            "Epoch [10/10], Step [9000/12500], Loss: 0.0918\n",
            "Epoch [10/10], Step [9200/12500], Loss: 0.0687\n",
            "Epoch [10/10], Step [9400/12500], Loss: 0.0381\n",
            "Epoch [10/10], Step [9600/12500], Loss: 0.0372\n",
            "Epoch [10/10], Step [9800/12500], Loss: 0.0473\n",
            "Epoch [10/10], Step [10000/12500], Loss: 0.0336\n",
            "Epoch [10/10], Step [10200/12500], Loss: 0.0596\n",
            "Epoch [10/10], Step [10400/12500], Loss: 0.0483\n",
            "Epoch [10/10], Step [10600/12500], Loss: 0.0560\n",
            "Epoch [10/10], Step [10800/12500], Loss: 0.0313\n",
            "Epoch [10/10], Step [11000/12500], Loss: 0.0563\n",
            "Epoch [10/10], Step [11200/12500], Loss: 0.0687\n",
            "Epoch [10/10], Step [11400/12500], Loss: 0.0927\n",
            "Epoch [10/10], Step [11600/12500], Loss: 0.0850\n",
            "Epoch [10/10], Step [11800/12500], Loss: 0.0997\n",
            "Epoch [10/10], Step [12000/12500], Loss: 0.0699\n",
            "Epoch [10/10], Step [12200/12500], Loss: 0.0463\n",
            "Epoch [10/10], Step [12400/12500], Loss: 0.0649\n",
            "Finished training\n",
            "Accuracy of the network on the 10000 test images: 74 %\n",
            "Freezing all layers and randomising fc3\n",
            "Saved frozen model with unfrozen fc3. Resetted fc3 parameters and stored in 'base.pt.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WtO5m4GSZRvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "I9sp4Q9MK1Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python de.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EQasROBITYo",
        "outputId": "2a7cc80e-d6f1-4ddb-97ce-a228f6e7b38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=8192, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            ")\n",
            "0\tTraceback (most recent call last):\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/de.py\", line 113, in <module>\n",
            "    print(train(base, 0.5, 0.5, POPULATION_SIZE, 1))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/de.py\", line 98, in train\n",
            "    print(loss_function(get_best(population)))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/de.py\", line 80, in get_best\n",
            "    return min(population, key=loss_function)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/loss_function.py\", line 159, in f\n",
            "    return loss_function(individual, base_model)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/loss_function.py\", line 128, in loss_function\n",
            "    return _loss_function(base_model)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/loss_function.py\", line 101, in _loss_function\n",
            "    loss_val, _ = eval_loss_and_acc(model)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/loss_function.py\", line 78, in eval_loss_and_acc\n",
            "    assert next(model.parameters()).device == device, \"Model not on correct device.\"\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: Model not on correct device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python nsgaii.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgdbID79IXRp",
        "outputId": "46fcb62f-64d1-4726-ce36-83fc99ae0e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/COMPUTATIONALINTELLIGENCEPERSONAL/nsgaii.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sqn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxpU1cyIIbPa",
        "outputId": "8510bb76-320c-4f81-e951-1c4470ef5c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=8192, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            ")\n",
            "tensor([-0.0302, -0.0441, -0.0336,  ..., -0.0267, -0.0281,  0.0040],\n",
            "       device='cuda:0')\n",
            "1\t2.317809820175171\n",
            "2\t1.7881377516459906e-06\n",
            "3\t1.1920928244535389e-07\n",
            "4\t0.0\n",
            "5\t0.0\n",
            "6\t0.16066507995128632\n",
            "7\t0.013991366140544415\n",
            "8\t0.000872946111485362\n",
            "9\t2.074220174108632e-05\n",
            "10\t7.152555099310121e-07\n",
            "11\t0.029162295162677765\n",
            "12\t0.00017391123401466757\n",
            "13\t0.0004033228906337172\n",
            "14\t0.0003829461056739092\n",
            "15\t0.06160463020205498\n",
            "16\t0.00017188502533826977\n",
            "17\t0.0\n",
            "18\t0.011260293424129486\n",
            "19\t0.0006936766440048814\n",
            "20\t5.1973900554003194e-05\n",
            "21\t1.5020257706055418e-05\n",
            "22\t0.0\n",
            "23\t0.001927543431520462\n",
            "24\t0.001462224405258894\n",
            "25\t9.894321920000948e-06\n",
            "26\t0.0033005783334374428\n",
            "27\t2.038458114839159e-05\n",
            "28\t0.008349984884262085\n",
            "29\t0.0\n",
            "30\t0.37910306453704834\n",
            "31\t7.033323527139146e-06\n",
            "32\t0.0\n",
            "33\t0.0006663962849415839\n",
            "34\t2.145764938177308e-06\n",
            "35\t1.1920928244535389e-07\n",
            "36\t7.283422019099817e-05\n",
            "37\t0.0\n",
            "38\t0.0\n",
            "39\t0.0020384264644235373\n",
            "40\t0.0041399020701646805\n",
            "41\t5.936446541454643e-05\n",
            "42\t2.861018856492592e-06\n",
            "43\t1.0967194612021558e-05\n",
            "44\t9.393251093570143e-05\n",
            "45\t2.8967437174287625e-05\n",
            "46\t0.008176662027835846\n",
            "47\t0.13962218165397644\n",
            "48\t0.011716952547430992\n",
            "49\t4.1960789531003684e-05\n",
            "50\t0.0\n",
            "51\t0.0\n",
            "52\t0.0023664822801947594\n",
            "53\t0.010747289285063744\n",
            "54\t0.0015985103091225028\n",
            "55\t0.030847718939185143\n",
            "56\t0.0\n",
            "57\t0.003643661504611373\n",
            "58\t2.3841830625315197e-06\n",
            "59\t0.0\n",
            "60\t1.1920928244535389e-07\n",
            "61\t0.0\n",
            "62\t9.536738616588991e-07\n",
            "63\t1.1920922133867862e-06\n",
            "64\t1.811964830267243e-05\n",
            "65\t1.1920928244535389e-07\n",
            "66\t0.0005289109540171921\n",
            "67\t5.8412379075889476e-06\n",
            "68\t0.008727265521883965\n",
            "69\t0.0\n",
            "70\t0.006956997327506542\n",
            "71\t1.1920928244535389e-07\n",
            "72\t0.08274199068546295\n",
            "73\t1.5497195136049413e-06\n",
            "74\t9.536738616588991e-07\n",
            "75\t0.015221036970615387\n",
            "76\t6.592056161025539e-05\n",
            "77\t0.00011789103882620111\n",
            "78\t0.0029332491103559732\n",
            "79\t0.029533080756664276\n",
            "80\t0.0002796259068418294\n",
            "81\t0.0\n",
            "82\t0.015833135694265366\n",
            "83\t0.0006068295333534479\n",
            "84\t0.028855307027697563\n",
            "85\t0.004324727226048708\n",
            "86\t0.001977513777092099\n",
            "87\t0.00011526874004630372\n",
            "88\t0.0\n",
            "89\t0.015524492599070072\n",
            "90\t2.932505594799295e-05\n",
            "91\t0.0001494772732257843\n",
            "92\t0.0851471871137619\n",
            "93\t0.01882028579711914\n",
            "94\t3.4450891689630225e-05\n",
            "95\t0.11989255994558334\n",
            "96\t1.1920928244535389e-07\n",
            "97\t1.0728830375228426e-06\n",
            "98\t0.0\n",
            "99\t0.0002867764269467443\n",
            "100\t0.0\n",
            "tensor([-0.1013, -0.1163, -0.0255,  ..., -0.0706, -0.0507,  0.0256],\n",
            "       device='cuda:0')\n",
            "CNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=8192, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf COMPUTATIONALINTELLIGENCEPERSONAL\n",
        "!git clone https://github.com/JKitsopanos/COMPUTATIONALINTELLIGENCEPERSONAL.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kch-HwsRaNHe",
        "outputId": "decd9a60-f3be-42e5-8072-86e901ceb39d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'COMPUTATIONALINTELLIGENCEPERSONAL'...\n",
            "remote: Enumerating objects: 106, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 106 (delta 53), reused 106 (delta 53), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (106/106), 178.82 KiB | 1.41 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf COMPUTATIONALINTELLIGENCEPERSONAL\n",
        "!git clone https://github.com/JKitsopanos/COMPUTATIONALINTELLIGENCEPERSONAL.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9dsMympalQW",
        "outputId": "519da827-478e-4307-b829-ad80c8b0e541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'COMPUTATIONALINTELLIGENCEPERSONAL'...\n",
            "remote: Enumerating objects: 106, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 106 (delta 53), reused 106 (delta 53), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (106/106), 178.82 KiB | 1.38 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd COMPUTATIONALINTELLIGENCEPERSONAL\n"
      ],
      "metadata": {
        "id": "E251rzlTaoPO",
        "outputId": "72505002-caf5-4a83-b08a-618726508ea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/COMPUTATIONALINTELLIGENCEPERSONAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()\n"
      ],
      "metadata": {
        "id": "1I-P6ItFap7m",
        "outputId": "b023e926-7f64-4ed1-a98d-0c255f9af9ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['references.bib',\n",
              " 'additional files',\n",
              " 'loss_function.py',\n",
              " 'main.py',\n",
              " 'sgd.py',\n",
              " 'cso.py',\n",
              " 'requirements_gpu.txt',\n",
              " 'visualisation.py',\n",
              " 'Makefile',\n",
              " 'requirements.txt',\n",
              " 'nsgaII.py',\n",
              " '.gitignore',\n",
              " 'readme.md',\n",
              " '.git',\n",
              " 'sqn.py',\n",
              " 'de.py',\n",
              " 'report.tex']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cso.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrsZfK-EI8mZ",
        "outputId": "dfc6eda0-8852-498a-ece8-ca5934eb098c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/cso.py\", line 192, in <module>\n",
            "    population_size=POPULATION_SIZE,\n",
            "                    ^^^^^^^^^^^^^^^\n",
            "NameError: name 'POPULATION_SIZE' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python de.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjeT9w6jI_ZY",
        "outputId": "e3822153-6b08-47dd-8ae6-74ef4c10b46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=8192, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            ")\n",
            "0\tTraceback (most recent call last):\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/de.py\", line 113, in <module>\n",
            "    print(train(base, 0.5, 0.5, POPULATION_SIZE, 1))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/de.py\", line 98, in train\n",
            "    print(loss_function(get_best(population)))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/de.py\", line 80, in get_best\n",
            "    return min(population, key=loss_function)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/loss_function.py\", line 159, in f\n",
            "    return loss_function(individual, base_model)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/loss_function.py\", line 128, in loss_function\n",
            "    return _loss_function(base_model)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/loss_function.py\", line 101, in _loss_function\n",
            "    loss_val, _ = eval_loss_and_acc(model)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/COMPUTATIONALINTELLIGENCEPERSONAL/loss_function.py\", line 78, in eval_loss_and_acc\n",
            "    assert next(model.parameters()).device == device, \"Model not on correct device.\"\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: Model not on correct device.\n"
          ]
        }
      ]
    }
  ]
}